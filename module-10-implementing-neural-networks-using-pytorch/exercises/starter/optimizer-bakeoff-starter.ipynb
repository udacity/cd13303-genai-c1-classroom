{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "087f2b36",
      "metadata": {},
      "source": [
        "# Exercise: Optimizer Bake-off!\n",
        "\n",
        "Let's compare common optimizers (SGD, Adam, RMSprop) on a small CNN using Fashion-MNIST.\n",
        "\n",
        "In this example we will use three dataset splits:\n",
        "* `Train`: examples used for training the model using backpropagation\n",
        "* `Validation`: examples used to evaluate model performance during training (e.g., to check for overfitting) and to choose the optimizer and hyperparameters\n",
        "* `Test`: examples used to report the model performance at the very end"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8adb37c7",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Imports and global config\n",
        "# No changes needed in this cell\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "import math, time, random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "\n",
        "set_seed(1234)\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3823a79f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Select device\n",
        "# No changes needed in this cell\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "518bac88",
      "metadata": {},
      "source": [
        "## Data transforms\n",
        "Now we define simple transforms for Fashion-MNIST (tensor + normalization)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ba1bca9",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Student task: define data transforms (train and test)\n",
        "# Complete the sections with **********\n",
        "\n",
        "# Hint: Use transforms.ToTensor() and transforms.Normalize((0.5,), (0.5,))\n",
        "# train_tfms = **********\n",
        "# test_tfms  = **********\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "80d06991",
      "metadata": {},
      "source": [
        "## Data loaders\n",
        "Now we load Fashion-MNIST and create train/validation/test data loaders."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "627fdf22",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load datasets and create dataloaders\n",
        "\n",
        "# No changes needed in this cell\n",
        "train_ds = datasets.FashionMNIST(\n",
        "    root=\"./data\", train=True, download=True, transform=train_tfms\n",
        ")\n",
        "test_ds = datasets.FashionMNIST(\n",
        "    root=\"./data\", train=False, download=True, transform=test_tfms\n",
        ")\n",
        "\n",
        "# Create a small validation split from the training set\n",
        "val_ratio = 0.1\n",
        "val_size = int(len(train_ds) * val_ratio)\n",
        "train_size = len(train_ds) - val_size\n",
        "train_ds, val_ds = torch.utils.data.random_split(\n",
        "    train_ds, [train_size, val_size], generator=torch.Generator().manual_seed(1234)\n",
        ")\n",
        "\n",
        "# Create Data loaders: train_loader, val_loader, test_loader\n",
        "\n",
        "# train_loader = DataLoader(**********)\n",
        "# val_loader   = DataLoader(**********)\n",
        "# test_loader  = DataLoader(**********)\n",
        "\n",
        "\n",
        "len(train_ds), len(val_ds), len(test_ds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5b2756d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Show some sample images\n",
        "# No changes needed in this cell\n",
        "\n",
        "\n",
        "def show_images(dataloader):\n",
        "    batch = next(iter(dataloader))\n",
        "    images, labels = batch\n",
        "    fig, ax = plt.subplots(3, 5, figsize=(5, 3))\n",
        "    for i in range(3):\n",
        "        for j in range(5):\n",
        "            ax[i, j].imshow(images[i * 5 + j].squeeze(), cmap=\"gray\")\n",
        "            ax[i, j].set_title(f\"Label: {labels[i * 5 + j].item()}\")\n",
        "            ax[i, j].axis(\"off\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "show_images(train_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f1fa4e3b",
      "metadata": {},
      "source": [
        "## Model\n",
        "Let's define a tiny CNN suitable for 28Ã—28 grayscale inputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c2628874",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Tiny CNN model definition\n",
        "# No changes needed in this cell\n",
        "\n",
        "\n",
        "class TinyCNN(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.drop = nn.Dropout(0.25)\n",
        "        self.fc1 = nn.Linear(32 * 7 * 7, 128)\n",
        "        self.fc2 = nn.Linear(128, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))  # 28->14\n",
        "        x = self.pool(F.relu(self.conv2(x)))  # 14->7\n",
        "        x = self.drop(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "model = TinyCNN().to(device)\n",
        "\n",
        "# Show the model size\n",
        "sum(p.numel() for p in model.parameters()), model.__class__.__name__"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1ab22919",
      "metadata": {},
      "source": [
        "## Metrics\n",
        "Now we implement a simple accuracy function given logits and labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "734c40cd",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Student task: Get the prediction values from logits and compute accuracy\n",
        "# Complete the sections with **********\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def accuracy_from_logits(logits, y):\n",
        "    \"\"\"Compute accuracy from model logits and true labels.\n",
        "\n",
        "    logits: (B, C), y: (B,)\n",
        "    \"\"\"\n",
        "\n",
        "    # Use argmax to get predicted class. Hint: dim = 1\n",
        "    preds = \"**********\"\n",
        "\n",
        "    # Get boolean tensor of correct predictions, hint: use ==\n",
        "    correct = \"**********\"\n",
        "\n",
        "\n",
        "    # Return mean accuracy of the batch\n",
        "    return correct.float().mean().item()\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d5f8a76c",
      "metadata": {},
      "source": [
        "## Training utilities\n",
        "Now we write the train loop body for one epoch and reuse it across optimizers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3e8f483",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Student task: complete train_one_epoch with zero_grad â†’ forward â†’ loss.backward â†’ optimizer.step\n",
        "# Complete the sections with **********\n",
        "\n",
        "\n",
        "def train_one_epoch(model, loader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    running_loss, running_acc, n = 0.0, 0.0, 0\n",
        "    for x, y in loader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "\n",
        "        # TODO: implement the training loop steps. See train_one_epoch in for a hint\n",
        "        # in https://docs.pytorch.org/tutorials/beginner/introyt/trainingyt.html\n",
        "        # **********  # zero the gradients\n",
        "        # logits = **********  # run the model on the inputs to get the logits\n",
        "        # loss = **********    # calculate the loss using criterion\n",
        "        # **********           # run backpropagation to compute gradients\n",
        "        # **********           # run a single optimization step\n",
        "\n",
        "        # stats\n",
        "        batch = x.size(0)\n",
        "        running_loss += loss.item() * batch\n",
        "        running_acc += accuracy_from_logits(logits, y) * batch\n",
        "        n += batch\n",
        "    return running_loss / n, running_acc / n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d0034ddf",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Validation/test loop\n",
        "# No changes needed in this cell\n",
        "@torch.no_grad()\n",
        "def evaluate(model, loader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss, total_acc, n = 0.0, 0.0, 0\n",
        "    for x, y in loader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        logits = model(x)\n",
        "        loss = criterion(logits, y)\n",
        "        batch = x.size(0)\n",
        "        total_loss += loss.item() * batch\n",
        "        total_acc += accuracy_from_logits(logits, y) * batch\n",
        "        n += batch\n",
        "    return total_loss / n, total_acc / n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "18235f75",
      "metadata": {},
      "source": [
        "## Optimizers & hyperparameters\n",
        "Now we set up three optimizers to compare (SGD, Adam, RMSprop)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e8ec7664",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Student task: create an optimizers dict with configs for 'sgd', 'adam', 'rmsprop'\n",
        "# Complete the sections with **********\n",
        "\n",
        "\n",
        "# Function to create a fresh model instance for each optimizer\n",
        "def make_fresh_model():\n",
        "    m = TinyCNN().to(device)\n",
        "    return m\n",
        "\n",
        "\n",
        "# We use CrossEntropyLoss for classification problems\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "# Function to create an optimizer for a given model\n",
        "def make_optimizer(name, params):\n",
        "    if name == \"sgd\":\n",
        "        return optim.SGD(params, **optim_cfgs[\"sgd\"])\n",
        "    if name == \"adam\":\n",
        "        return optim.Adam(params, **optim_cfgs[\"adam\"])\n",
        "    if name == \"rmsprop\":\n",
        "        return optim.RMSprop(params, **optim_cfgs[\"rmsprop\"])\n",
        "    raise ValueError(\"Unknown optimizer\")\n",
        "\n",
        "\n",
        "# Optimizer configurations\n",
        "# optim_cfgs = {\n",
        "#     'sgd':     {'lr': **********, 'momentum': **********, 'weight_decay': ********__},\n",
        "#     'adam':    {'lr': ********__, 'betas': ********__,    'weight_decay': ********__},\n",
        "#     'rmsprop': {'lr': ********__, 'alpha': ********__,    'weight_decay': ********__},\n",
        "# }\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "08f22cd5",
      "metadata": {},
      "source": [
        "## Training\n",
        "Now we train each optimizer for a few epochs and record train/val metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ffa2607a",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train each optimizer for a few epochs and store histories\n",
        "# No changes needed in this cell\n",
        "EPOCHS = 3  # keep short for classroom runtime\n",
        "histories = {}\n",
        "\n",
        "for opt_name in [\"sgd\", \"adam\", \"rmsprop\"]:\n",
        "    print(f\"\\n=== Training with {opt_name.upper()} ===\")\n",
        "    set_seed(1234)  # reset for fair comparison\n",
        "    model = make_fresh_model()\n",
        "    optimizer = make_optimizer(opt_name, model.parameters())\n",
        "\n",
        "    train_losses, train_accs = [], []\n",
        "    val_losses, val_accs = [], []\n",
        "\n",
        "    for epoch in range(1, EPOCHS + 1):\n",
        "        tl, ta = train_one_epoch(model, train_loader, criterion, optimizer, device)\n",
        "        vl, va = evaluate(model, val_loader, criterion, device)\n",
        "        train_losses.append(tl)\n",
        "        train_accs.append(ta)\n",
        "        val_losses.append(vl)\n",
        "        val_accs.append(va)\n",
        "        print(\n",
        "            f\"Epoch {epoch:02d} | train loss {tl:.4f} acc {ta:.4f} | val loss {vl:.4f} acc {va:.4f}\"\n",
        "        )\n",
        "\n",
        "    histories[opt_name] = {\n",
        "        \"train_loss\": train_losses,\n",
        "        \"train_acc\": train_accs,\n",
        "        \"val_loss\": val_losses,\n",
        "        \"val_acc\": val_accs,\n",
        "    }\n",
        "\n",
        "print(\"\\nDone training all optimizers.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d116e55c",
      "metadata": {},
      "source": [
        "## Results\n",
        "Now we plot validation accuracy curves to compare optimizers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f408524",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot validation accuracy curves\n",
        "# No changes needed in this cell\n",
        "plt.figure(figsize=(7, 4))\n",
        "for name, h in histories.items():\n",
        "    plt.plot(range(1, len(h[\"val_acc\"]) + 1), h[\"val_acc\"], label=name)\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Validation Accuracy\")\n",
        "plt.title(\"Optimizer Bake-off: Val Accuracy\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "284787a1",
      "metadata": {},
      "source": [
        "## Final evaluation\n",
        "Now we pick the best-performing optimizer by validation accuracy and report test accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dfc672bc",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate the best optimizer on the test set\n",
        "# No changes needed in this cell\n",
        "best_name = max(histories.keys(), key=lambda n: max(histories[n][\"val_acc\"]))\n",
        "print(\"Best by val acc:\", best_name)\n",
        "\n",
        "set_seed(1234)\n",
        "best_model = make_fresh_model()\n",
        "best_opt = make_optimizer(best_name, best_model.parameters())\n",
        "for _ in range(3):\n",
        "    train_one_epoch(best_model, train_loader, criterion, best_opt, device)\n",
        "_, test_acc = evaluate(best_model, test_loader, criterion, device)\n",
        "print(f\"Test accuracy with {best_name.upper()}: {test_acc:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7a828fa6",
      "metadata": {},
      "source": [
        "## Great job! ðŸŽ‰ðŸŽ‰ðŸŽ‰\n",
        "\n",
        "You've completed the optimizer bake-off exercise. Understanding how different optimizers affect training dynamics is crucial for building effective neural networks. Keep experimenting!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e972254b",
      "metadata": {},
      "source": [
        "<br /><br /><br /><br /><br /><br /><br /><br /><br />"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}