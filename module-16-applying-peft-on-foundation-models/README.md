# Applying PEFT on Foundation Models

This module introduces Parameter-Efficient Fine-Tuning (PEFT) techniques for adapting large foundation models to specific tasks. You will learn to use methods like LoRA, AdaLoRA, and QLoRA to fine-tune models efficiently while using minimal computational resources and maintaining model performance.

## Folder Structure

```
> tree
.
|-- README.md
`-- exercises
    |-- README.md
    |-- solution
    |   `-- README.md
    `-- starter
        `-- README.md

4 directories, 4 files
```

## Directory Map
- `exercises/README.md` – overview of PEFT concepts and fine-tuning strategies
- `exercises/starter/` – PEFT implementation starter code and training datasets
- `exercises/solution/` – complete fine-tuning pipeline with LoRA adapters and evaluation metrics

## Additional Resources
- [LoRA Paper](https://arxiv.org/abs/2106.09685) - Original Low-Rank Adaptation research
- [PEFT Library Documentation](https://huggingface.co/docs/peft/) - Comprehensive guide to PEFT methods

