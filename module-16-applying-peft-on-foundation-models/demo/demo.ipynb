{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "demo-intro-1",
   "metadata": {},
   "source": [
    "# Demo: Teach an LLM a New Skill with SFT\n",
    "\n",
    "Welcome! This notebook is a short demonstration to show you how to teach a Large Language Model (LLM) a new skill using Supervised Fine-Tuning (SFT). \n",
    "\n",
    "LLMs are great at many things, but they don't know everything. Sometimes, we need to teach them a specific, new task. In this demo, we'll teach a small LLM to add the suffish \"-ish\" to the ends of words.\n",
    "\n",
    "This demo follows the exact same structure as the exercise you're about to do. Pay attention to the steps, as you'll be repeating them to teach the model how to spell."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demo-outline-2",
   "metadata": {},
   "source": [
    "## What you'll see in this demo\n",
    "\n",
    "1.  **Setup**: Import libraries and configure the environment.\n",
    "2.  **Load the model**: Use a small, instruction-tuned model as our starting point.\n",
    "3.  **Create a dataset**: Generate a simple dataset of words and their -ish variants.\n",
    "4.  **Evaluate the base model**: See how the model does *before* any training.\n",
    "5.  **Configure LoRA and train**: Use Parameter-Efficient Fine-Tuning (PEFT) with LoRA to train our model efficiently.\n",
    "6.  **Evaluate the fine-tuned model**: Test the model again to see its new skill in action."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demo-setup-hdr",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "demo-setup-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# Setup necessary imports\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "# Use GPU, MPS, or CPU, depending on what's available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "torch.set_num_threads(max(1, os.cpu_count() // 2))\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demo-load-model-hdr",
   "metadata": {},
   "source": [
    "## Step 1. Load the tokenizer and base model\n",
    "\n",
    "We'll use `HuggingFaceTB/SmolLM2-135M-Instruct`, a small model with 135 million parameters. Its small size makes it perfect for a quick demonstration on a standard computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "demo-load-model-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded: HuggingFaceTB/SmolLM2-135M-Instruct\n"
     ]
    }
   ],
   "source": [
    "# Model ID from Hugging Face\n",
    "model_id = \"HuggingFaceTB/SmolLM2-135M-Instruct\"\n",
    "\n",
    "# Load the tokenizer, which prepares text for the model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# Load the model itself\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "\n",
    "# Move the model to our selected device (GPU/CPU)\n",
    "model = model.to(device)\n",
    "\n",
    "print(f\"Model loaded: {model_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demo-create-ds-hdr",
   "metadata": {},
   "source": [
    "## Step 2. Create the dataset\n",
    "\n",
    "Next, we'll create a small dataset of examples to teach the model our new task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "demo-word-list",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset will be created from 62 words.\n"
     ]
    }
   ],
   "source": [
    "# A list of words for our demonstration\n",
    "\n",
    "# fmt: off\n",
    "DEMO_WORDS = [\n",
    "    \"idea\", \"glow\", \"rust\", \"maze\", \"echo\", \"wisp\", \"veto\", \"lush\", \"gaze\", \"knit\", \"fume\", \"plow\",\n",
    "    \"void\", \"oath\", \"grim\", \"crisp\", \"lunar\", \"fable\", \"quest\", \"verge\", \"brawn\", \"elude\", \"aisle\",\n",
    "    \"ember\", \"crave\", \"ivory\", \"mirth\", \"knack\", \"wryly\", \"onset\", \"mosaic\", \"velvet\", \"sphinx\",\n",
    "    \"radius\", \"summit\", \"banner\", \"cipher\", \"glisten\", \"mantle\", \"scarab\", \"expose\", \"fathom\",\n",
    "    \"tavern\", \"fusion\", \"relish\", \"lantern\", \"enchant\", \"torrent\", \"capture\", \"orchard\", \"eclipse\",\n",
    "    \"frescos\", \"triumph\", \"absolve\", \"gossipy\", \"prelude\", \"whistle\", \"resolve\", \"zealous\",\n",
    "    \"mirage\", \"aperture\", \"sapphire\",\n",
    "]\n",
    "# fmt: on\n",
    "\n",
    "print(f\"Dataset will be created from {len(DEMO_WORDS)} words.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "demo-generate-records",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First training example:\n",
      "{'prompt': 'Add -ish to the end of the word.\\nhello -> hello-ish\\nlearn -> learn-ish\\nivory -> ', 'completion': 'ivory-ish'}\n"
     ]
    }
   ],
   "source": [
    "# This function creates prompt/completion pairs for our dataset.\n",
    "def generate_records():\n",
    "    for word in DEMO_WORDS:\n",
    "        # The prompt tells the model what to do.\n",
    "        prompt = (\n",
    "            f\"Add -ish to the end of the word.\\n\"\n",
    "            \"hello -> hello-ish\\n\"\n",
    "            \"learn -> learn-ish\\n\"\n",
    "            f\"{word} -> \"\n",
    "        )\n",
    "        # The completion is the correct answer.\n",
    "        completion = f\"{word}-ish\"\n",
    "        yield {\"prompt\": prompt, \"completion\": completion}\n",
    "\n",
    "\n",
    "# Create a Hugging Face Dataset from our generator\n",
    "ds = Dataset.from_generator(generate_records)\n",
    "\n",
    "# Split the dataset: 80% for training, 20% for testing\n",
    "ds = ds.train_test_split(test_size=0.2, seed=42)\n",
    "\n",
    "# Let's look at the first training example\n",
    "print(\"First training example:\")\n",
    "print(ds[\"train\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demo-eval-base-hdr",
   "metadata": {},
   "source": [
    "## Step 3. Evaluate the base model\n",
    "\n",
    "Before we train, let's see if the model already knows how to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "demo-check-translation-func",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A helper function to test the model's translation ability\n",
    "def check_translation(model, tokenizer, prompt: str, actual_translation: str):\n",
    "    # Prepare the input for the model\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    # Generate a response from the model\n",
    "    gen = model.generate(**inputs, max_new_tokens=15, use_cache=False)\n",
    "    output = tokenizer.decode(gen[0], skip_special_tokens=True)\n",
    "\n",
    "    # Extract just the translated part\n",
    "    proposed_translation = output.split(\"->\")[-1].strip().split(\"\\n\")[0].strip()\n",
    "\n",
    "    # Check if the model's answer is correct\n",
    "    is_correct = proposed_translation == actual_translation\n",
    "    print(\n",
    "        f\"Proposed: {proposed_translation} | Actual: {actual_translation} \"\n",
    "        f\"| Correct: {'‚úÖ' if is_correct else '‚ùå'}\"\n",
    "    )\n",
    "    return is_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "demo-eval-base-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Evaluating Base Model (Before Training) ---\n",
      "Proposed: wryly-ish | Actual: wryly-ish | Correct: ‚úÖ\n",
      "Proposed: 1-ish | Actual: glisten-ish | Correct: ‚ùå\n",
      "Proposed: quest-ish | Actual: quest-ish | Correct: ‚úÖ\n",
      "Proposed: ire-ish | Actual: crave-ish | Correct: ‚ùå\n",
      "Proposed: ils-ish | Actual: lush-ish | Correct: ‚ùå\n",
      "Proposed: —Ñ–∞–π–ª–µ | Actual: fable-ish | Correct: ‚ùå\n",
      "Proposed: knack-ish | Actual: knack-ish | Correct: ‚úÖ\n",
      "Proposed: iumph-ish | Actual: triumph-ish | Correct: ‚ùå\n",
      "Proposed: sapphire-ish | Actual: sapphire-ish | Correct: ‚úÖ\n",
      "Proposed: expose-ish | Actual: expose-ish | Correct: ‚úÖ\n",
      "Proposed: ils-es | Actual: frescos-ish | Correct: ‚ùå\n",
      "Proposed: wisp-ish | Actual: wisp-ish | Correct: ‚úÖ\n",
      "Proposed: mirage-ish | Actual: mirage-ish | Correct: ‚úÖ\n",
      "\n",
      "Result: 7/13 correct.\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Evaluating Base Model (Before Training) ---\")\n",
    "num_correct = 0\n",
    "num_examples = len(ds[\"test\"])\n",
    "\n",
    "for example in ds[\"test\"]:\n",
    "    prompt = example[\"prompt\"]\n",
    "    completion = example[\"completion\"]\n",
    "    if check_translation(model, tokenizer, prompt, completion):\n",
    "        num_correct += 1\n",
    "\n",
    "print(f\"\\nResult: {num_correct}/{num_examples} correct.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demo-lora-hdr",
   "metadata": {},
   "source": [
    "The base model does OK, but it can do better.\n",
    "\n",
    "## Step 4. Configure LoRA and train the model\n",
    "\n",
    "We'll use Low-Rank Adaptation (LoRA) to make training fast and memory-efficient. LoRA adds a small number of new, trainable parameters to the model, freezing the original ones. This means we only have to update a tiny fraction of the model's weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "demo-lora-config",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params: 3,686,400 / 138,201,408 (2.67%)\n"
     ]
    }
   ],
   "source": [
    "# LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=64,  # Rank of the update matrices. Lower is fewer parameters.\n",
    "    lora_alpha=16,  # LoRA scaling factor. Generally set to 16.\n",
    "    lora_dropout=0.05,  # Dropout for LoRA layers\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "# Wrap the base model with LoRA layers\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Print the percentage of trainable parameters\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(\n",
    "    f\"Trainable params: {trainable_params:,} / {total_params:,} ({100 * trainable_params / total_params:.2f}%)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demo-training-args-hdr",
   "metadata": {},
   "source": [
    "Notice that we're only training about 2.67% of the total parameters! Now, we set the training arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "demo-sftconfig",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training arguments for the SFTTrainer\n",
    "training_args = SFTConfig(\n",
    "    output_dir=\"data/model_demo\",  # Directory to save artifacts\n",
    "    per_device_train_batch_size=8,  # Small batch size for demo\n",
    "    gradient_accumulation_steps=2,  # Two forward and backward passes per update step\n",
    "    num_train_epochs=20,  # Number of times to go through the data\n",
    "    learning_rate=2e-4,  # Controls how much the model weights are updated\n",
    "    logging_steps=50,  # Log training progress every 10 steps\n",
    "    save_strategy=\"no\",  # Don't save model checkpoints\n",
    "    report_to=[],  # Disable reporting to services like Weights & Biases\n",
    "    fp16=False,  # Use full precision (fp32) for wider compatibility\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "demo-trainer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Training ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0341d830b46a4f739e67a1623e124e7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/briancruz/udacity/udacity genai nd c1 project refresh 2025/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3888, 'grad_norm': 0.05742060765624046, 'learning_rate': 3.3333333333333335e-05, 'num_tokens': 21162.0, 'mean_token_accuracy': 0.9018999320268631, 'epoch': 14.29}\n",
      "{'train_runtime': 14.9863, 'train_samples_per_second': 65.393, 'train_steps_per_second': 4.004, 'train_loss': 0.32870734483003616, 'num_tokens': 25352.0, 'mean_token_accuracy': 1.0, 'epoch': 17.14}\n",
      "--- Training Complete ---\n"
     ]
    }
   ],
   "source": [
    "# Create the SFTTrainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=ds[\"train\"],\n",
    "    eval_dataset=ds[\"test\"],\n",
    "    args=training_args,\n",
    ")\n",
    "\n",
    "# Start the training process!\n",
    "print(\"--- Starting Training ---\")\n",
    "trainer.train()\n",
    "print(\"--- Training Complete ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demo-eval-tuned-hdr",
   "metadata": {},
   "source": [
    "## Step 5. Evaluate the fine-tuned model\n",
    "\n",
    "Training is done! Now for the moment of truth. Let's see if our model learned the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "demo-eval-tuned-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Evaluating Fine-Tuned Model (After Training) ---\n",
      "Proposed: wryly-ish | Actual: wryly-ish | Correct: ‚úÖ\n",
      "Proposed: 1-ish | Actual: glisten-ish | Correct: ‚ùå\n",
      "Proposed: quest-ish | Actual: quest-ish | Correct: ‚úÖ\n",
      "Proposed: crave-ish | Actual: crave-ish | Correct: ‚úÖ\n",
      "Proposed: lus-ish | Actual: lush-ish | Correct: ‚ùå\n",
      "Proposed: fable-ish | Actual: fable-ish | Correct: ‚úÖ\n",
      "Proposed: knack-ish | Actual: knack-ish | Correct: ‚úÖ\n",
      "Proposed: t-m-i-p-h-e-l-o | Actual: triumph-ish | Correct: ‚ùå\n",
      "Proposed: sapphire-ish | Actual: sapphire-ish | Correct: ‚úÖ\n",
      "Proposed: expose-ish | Actual: expose-ish | Correct: ‚úÖ\n",
      "Proposed: Frescos-ish | Actual: frescos-ish | Correct: ‚ùå\n",
      "Proposed: wisp-ish | Actual: wisp-ish | Correct: ‚úÖ\n",
      "Proposed: mirage-ish | Actual: mirage-ish | Correct: ‚úÖ\n",
      "\n",
      "Result: 9/13 correct.\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Evaluating Fine-Tuned Model (After Training) ---\")\n",
    "num_correct = 0\n",
    "num_examples = len(ds[\"test\"])\n",
    "\n",
    "for example in ds[\"test\"]:\n",
    "    prompt = example[\"prompt\"]\n",
    "    completion = example[\"completion\"]\n",
    "    if check_translation(model, tokenizer, prompt, completion):\n",
    "        num_correct += 1\n",
    "\n",
    "print(f\"\\nResult: {num_correct}/{num_examples} correct.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demo-conclusion",
   "metadata": {},
   "source": [
    "## Conclusion üéâ\n",
    "\n",
    "Success! After a very short training run on a tiny dataset, the model improved on the task. It went from 7/13 to 9/13, which is modest, but shows fine-tuning with parameter-efficient fine-tuning works."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a13f83b",
   "metadata": {},
   "source": [
    "<br /><br /><br /><br /><br /><br /><br /><br />"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
