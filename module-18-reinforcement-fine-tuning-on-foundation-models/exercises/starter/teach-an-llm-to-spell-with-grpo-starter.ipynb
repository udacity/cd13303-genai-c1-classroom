{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e672bb19",
      "metadata": {},
      "source": [
        "# Exercise: Teach an LLM to Spell with Group Relative Policy Optimization (GRPO)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e01691a",
      "metadata": {},
      "source": [
        "Large language models (LLMs) are notoriously bad at spelling. This is partly because tokenizers break words into smaller pieces, so the model learns about sub-word units rather than whole words and their spellings.\n",
        "\n",
        "In this exercise, you'll use Group Relative Policy Optimization (GRPO) and a technique called Parameter-Efficient Fine-Tuning (PEFT) with Low-Rank Adaptation (LoRA) to teach a small LLM how to spell words. This is a classic example of teaching a model a new skill that isn't well-represented in its pre-training data.\n",
        "\n",
        "## What you'll do in this notebook\n",
        "\n",
        "1.  **Setup**: Import libraries and configure the environment.\n",
        "2.  **Load the tokenizer and base model**: Use a small, instruction-tuned model as our starting point.\n",
        "3.  **Create the dataset**: Generate a simple dataset of words and their correct spellings.\n",
        "4.  **Evaluate the base model**: Test the model's spelling ability *before* fine-tuning to establish a baseline.\n",
        "5.  **Configure LoRA and train**: Attach a LoRA adapter to the model and fine-tune it on the spelling dataset.\n",
        "6.  **Evaluate the fine-tuned model**: Test the model again to see if its spelling has improved."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d04085e7",
      "metadata": {},
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97437029",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup imports\n",
        "# No changes needed in this cell\n",
        "\n",
        "import os\n",
        "import torch\n",
        "from datasets import Dataset\n",
        "\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model\n",
        "from trl import SFTTrainer, SFTConfig\n",
        "\n",
        "# Use GPU, MPS, or CPU, in that order of preference\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")  # NVIDIA GPU\n",
        "elif torch.backends.mps.is_available():\n",
        "    device = torch.device(\"mps\")  # Apple Silicon\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "torch.set_num_threads(max(1, os.cpu_count() // 2))\n",
        "print(\"Using device:\", device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "18f0a0d7",
      "metadata": {},
      "source": [
        "## Step 1. Load the tokenizer and base model\n",
        "\n",
        "The model `HuggingFaceTB/SmolLM2-135M-Instruct` is a small, instruction-tuned model that's suitable for this exercise. It has 135 million parameters, making it lightweight and efficient for fine-tuning. It's not the most powerful model, but it's a good choice for demonstrating the concepts of SFT and PEFT with LoRA, especially on a CPU or limited GPU resources."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f8028ac1",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Student task: Load the model and tokenizer, and copy the model to the device.\n",
        "# TODO: Complete the sections with **********\n",
        "\n",
        "# See: https://huggingface.co/docs/transformers/en/models\n",
        "# See: https://huggingface.co/docs/transformers/en/fast_tokenizers\n",
        "\n",
        "# Model ID for SmolLM2-135M-Instruct\n",
        "model_id = \"***********\"\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = \"***********\"\n",
        "\n",
        "# Load the model\n",
        "model = \"***********\"\n",
        "\n",
        "# Copy the model to the device (GPU, MPS, or CPU)\n",
        "model = \"***********\"\n",
        "\n",
        "\n",
        "\n",
        "print(\"Model parameters (total):\", sum(p.numel() for p in model.parameters()))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c6665787",
      "metadata": {},
      "source": [
        "## Step 2. Create the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "46de84a5",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a list of words of different lengths\n",
        "# No changes are needed in this cell.\n",
        "\n",
        "# fmt: off\n",
        "ALL_WORDS = [\n",
        "    \"idea\", \"glow\", \"rust\", \"maze\", \"echo\", \"wisp\", \"veto\", \"lush\", \"gaze\", \"knit\", \"fume\", \"plow\",\n",
        "    \"void\", \"oath\", \"grim\", \"crisp\", \"lunar\", \"fable\", \"quest\", \"verge\", \"brawn\", \"elude\", \"aisle\",\n",
        "    \"ember\", \"crave\", \"ivory\", \"mirth\", \"knack\", \"wryly\", \"onset\", \"mosaic\", \"velvet\", \"sphinx\",\n",
        "    \"radius\", \"summit\", \"banner\", \"cipher\", \"glisten\", \"mantle\", \"scarab\", \"expose\", \"fathom\",\n",
        "    \"tavern\", \"fusion\", \"relish\", \"lantern\", \"enchant\", \"torrent\", \"capture\", \"orchard\", \"eclipse\",\n",
        "    \"frescos\", \"triumph\", \"absolve\", \"gossipy\", \"prelude\", \"whistle\", \"resolve\", \"zealous\",\n",
        "    \"mirage\", \"aperture\", \"sapphire\",\n",
        "]\n",
        "# fmt: on"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bbeab6e5",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Student Task: Create a Hugging Face Dataset with the prompt that asks the model to spell the word\n",
        "# with hyphens between the letters.\n",
        "# TODO: Complete the sections with **********\n",
        "\n",
        "\n",
        "def generate_records():\n",
        "    for word in ALL_WORDS:\n",
        "        yield {\n",
        "            # We will use the GRPOTrainer which expects to receieve formatted prompts\n",
        "            # to pass to the LLM\n",
        "            # https://huggingface.co/docs/trl/main/en/grpo_trainer\n",
        "            # \"**********\": f\"**********\",\n",
        "            # Before using GRPOTrainer, will run a few epochs of supervised-fine tuning (SFT)\n",
        "            # which can be useful to give an initial nudge to the model. Thus we need to provide\n",
        "            # the gold standard answer.\n",
        "            # See the documentation for more details:\n",
        "            # https://huggingface.co/docs/trl/en/sft_trainer#expected-dataset-type-and-format\n",
        "            # \"**********\": \"-\".join(word).upper() + \".\",\n",
        "            # GRPOTrainer does not expect a completion, but we can add extra columns to our dataset\n",
        "            # that our reward functions will use to grade the completions provided by the LLM.\n",
        "            \"word\": word,\n",
        "            \"spelling\": \"-\".join(word).upper(),\n",
        "        }\n",
        "\n",
        "\n",
        "ds = Dataset.from_generator(generate_records)\n",
        "\n",
        "ds = ds.train_test_split(test_size=0.1, seed=42)\n",
        "\n",
        "# Show the first item of the train split\n",
        "ds[\"train\"][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b6a9436",
      "metadata": {},
      "source": [
        "## Step 3. Evaluate the base model\n",
        "\n",
        "Before we fine-tune the model, let's see how it performs on the spelling task. We'll create a helper function to generate a spelling for a given word and compare it to the correct answer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6581c243",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a helper function that will help us visualize the performance of the model\n",
        "# No changes needed in this cell\n",
        "\n",
        "\n",
        "def check_spelling(\n",
        "    model, tokenizer, prompt: str, actual_spelling: str, max_new_tokens: int = 20\n",
        ") -> (str, str):\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "    gen = model.generate(\n",
        "        **inputs, max_new_tokens=max_new_tokens\n",
        "    )  # No parameters = greedy search\n",
        "    output = tokenizer.decode(gen[0], skip_special_tokens=True)\n",
        "\n",
        "    # Extract the generated spelling from the full output string\n",
        "    proposed_spelling = output.split(\"Spelling:\\n\")[-1].strip().split(\"\\n\")[0].strip()\n",
        "\n",
        "    # strip any whitepsace from the actual spelling\n",
        "    actual_spelling = actual_spelling.strip()\n",
        "\n",
        "    print(\n",
        "        f\"Proposed: {proposed_spelling} | Actual: {actual_spelling} \"\n",
        "        f\"| Matches: {'✅' if proposed_spelling == actual_spelling else '❌'}\"\n",
        "    )\n",
        "\n",
        "    # Remove hyphens for a character-by-character comparison\n",
        "    proposed_spelling = proposed_spelling.replace(\"-\", \"\")\n",
        "    actual_spelling = actual_spelling.replace(\"-\", \"\")\n",
        "\n",
        "    # Calculate the proportion of the spelling that was correct\n",
        "    num_correct = sum(1 for a, b in zip(actual_spelling, proposed_spelling) if a == b)\n",
        "\n",
        "    return num_correct / len(actual_spelling)  # Return proportion correct\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7642646c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Student task: Evaluate the base model's spelling ability\n",
        "# We expect it to perform poorly, as it hasn't been trained for this task.\n",
        "\n",
        "proportion_correct = 0.0\n",
        "\n",
        "for example in ds[\"train\"].select(range(20)):\n",
        "    prompt = example[\"prompt\"]\n",
        "    spelling = example[\"spelling\"]\n",
        "    result = check_spelling(\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        prompt=prompt,\n",
        "        actual_spelling=spelling,\n",
        "        max_new_tokens=20,\n",
        "    )\n",
        "    proportion_correct += result\n",
        "\n",
        "print(f\"{proportion_correct}/20.0 words correct\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e7c6563f",
      "metadata": {},
      "source": [
        "As expected, the base model is terrible at spelling. It mostly just repeats the word back. Now, let's fine-tune it."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d1e7ef15",
      "metadata": {},
      "source": [
        "## Step 4. Configure LoRA and train the model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "403a39e8",
      "metadata": {},
      "source": [
        "Let’s attach a LoRA adapter to the base model. We use a LoRA config so only a tiny fraction of parameters are trainable. Read more here: [LoRA](https://huggingface.co/docs/peft/main/en/conceptual_guides/lora)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d1b8d596",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Student task: Configure LoRA for a causal LM and wrap the model with get_peft_model\n",
        "# Complete the sections with **********\n",
        "\n",
        "# Print how many params are trainable at first\n",
        "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "total = sum(p.numel() for p in model.parameters())\n",
        "print(\n",
        "    f\"Trainable params BEFORE: {trainable:,} / {total:,} ({100 * trainable / total:.2f}%)\"\n",
        ")\n",
        "\n",
        "# See: https://huggingface.co/docs/peft/package_reference/lora\n",
        "# lora_config = LoraConfig(\n",
        "#     r=**********,                 # Rank of the update matrices. Lower value = fewer trainable parameters.\n",
        "#     lora_alpha=**********,        # LoRA scaling factor.\n",
        "#     lora_dropout=**********,      # Dropout probability for LoRA layers.\n",
        "#     bias=\"none\",\n",
        "#     task_type=**********,         # Causal Language Modeling.\n",
        "# )\n",
        "# # Wrap the base model with get_peft_model\n",
        "# model = get_peft_model(**********, **********)\n",
        "\n",
        "\n",
        "# Print the number of trainable parameters after applying LoRA\n",
        "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "total = sum(p.numel() for p in model.parameters())\n",
        "print(\n",
        "    f\"Trainable params AFTER: {trainable:,} / {total:,} ({100 * trainable / total:.2f}%)\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "30c5e91b",
      "metadata": {},
      "source": [
        "Now let’s set the training arguments. We'll use `SFTConfig` from the TRL library, which is a wrapper around the standard `TrainingArguments`. We keep epochs, batch size, and sequence length modest to finish training quickly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9341ba79",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train the model for a few epochs using SFT before GRPO as in certain cases\n",
        "# they can work together synergystically.\n",
        "# See: https://arxiv.org/html/2507.08267v1\n",
        "# No changes needed here\n",
        "\n",
        "output_dir = \"data/model\"\n",
        "\n",
        "training_args = SFTConfig(\n",
        "    output_dir=output_dir,\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    gradient_accumulation_steps=2,\n",
        "    num_train_epochs=10,\n",
        "    learning_rate=5 * 1e-4,\n",
        "    logging_steps=20,\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=20,\n",
        "    save_strategy=\"no\",\n",
        "    report_to=[],\n",
        "    fp16=False,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        ")\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=ds[\"train\"],\n",
        "    eval_dataset=ds[\"test\"],\n",
        "    args=training_args,\n",
        ")\n",
        "trainer.train()\n",
        "\n",
        "proportion_correct = 0.0\n",
        "\n",
        "for example in ds[\"train\"].select(range(20)):\n",
        "    prompt = example[\"prompt\"]\n",
        "    spelling = example[\"spelling\"]\n",
        "    result = check_spelling(\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        prompt=prompt,\n",
        "        actual_spelling=spelling,\n",
        "        max_new_tokens=20,\n",
        "    )\n",
        "    proportion_correct += result\n",
        "\n",
        "print(f\"{proportion_correct}/20.0 words correct\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2d995e64",
      "metadata": {},
      "source": [
        "The number of words has slightly increased. Let's try training using GRPO now.\n",
        "\n",
        "First let's create some reward functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd8e58bf",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Student Task: Create a helper function proportion_correct that takes a word and\n",
        "# a proposed spelling from the LLM and returns a score where every matched character\n",
        "# adds +1 and every  mismatched character subtracts 1 from the reward--including the\n",
        "# hyphens.\n",
        "# TODO: Replace occurences of **********\n",
        "\n",
        "import re\n",
        "\n",
        "\n",
        "def proportion_correct(word, proposed_spelling):\n",
        "    correct_spelling = \"-\".join(word).upper()\n",
        "\n",
        "    score = 0.0\n",
        "\n",
        "    # Pad to the same length to handle extra characters\n",
        "    max_len = max(len(correct_spelling), len(proposed_spelling))\n",
        "    proposed_spelling_padded = proposed_spelling.ljust(max_len, \" \")\n",
        "    correct_spelling_padded = correct_spelling.ljust(max_len, \" \")\n",
        "\n",
        "    for a, b in zip(correct_spelling_padded, proposed_spelling_padded):\n",
        "        # Add 1 for matched characters, and subtract one for mismatched\n",
        "        # **********\n",
        "\n",
        "\n",
        "    return score / (\n",
        "        len(correct_spelling)\n",
        "    )  # Normalize by length of spelling, including dashes\n",
        "\n",
        "\n",
        "assert proportion_correct(\"hello\", \"H-E-L-L-O\") == 9 / 9\n",
        "assert proportion_correct(\"hello\", \"H-E-L-\") == 3 / 9\n",
        "assert proportion_correct(\"hello\", \"H-E-L-L-O!\") == 8 / 9\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10e1457a",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a `reward_spelling` function that receives a batch of completions and the associated word values\n",
        "# No changes needed here\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def reward_spelling(completions, word, **kwargs):\n",
        "    \"\"\"Reward function that rewards completions with more unique letters.\"\"\"\n",
        "\n",
        "    completion_strings = [\n",
        "        completion.split(\"\\n\")[0].strip() for completion in completions\n",
        "    ]\n",
        "    words = [w for w in word]\n",
        "\n",
        "    rewards = [proportion_correct(w, c) for w, c in zip(words, completion_strings)]\n",
        "\n",
        "    # When training, GRPO will pass multiple completions and words to this function.\n",
        "    # We print just the first one to observe what is happening under the hood.\n",
        "    print(\"=====\")\n",
        "    print(\n",
        "        \"Completion example first line:\",\n",
        "        words[0],\n",
        "        \"->\",\n",
        "        completion_strings[0].strip().split(\"\\n\")[0].strip(),\n",
        "    )\n",
        "    print(f\"Spelling mean and std: {np.mean(rewards):.3f} +/- {np.std(rewards):.3f}\")\n",
        "    return rewards\n",
        "\n",
        "\n",
        "assert reward_spelling(\n",
        "    completions=[\n",
        "        \"H-E-L-L-O\",\n",
        "        \"H-E-L-\",\n",
        "        \"H-E-L-L-O!\",\n",
        "    ],\n",
        "    word=[\n",
        "        \"hello\",\n",
        "        \"hello\",\n",
        "        \"hello\",\n",
        "    ],\n",
        ") == [1, 3 / 9, 8 / 9]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b98d1ba",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Student task: Create a reward of 1.0 for completions starting with a string\n",
        "# formatted like X-Y-Z else return 0.0\n",
        "# TODO: Replace sections marked with **********\n",
        "\n",
        "\n",
        "def reward_response_in_form_of_letter_dash_letter(completions, word, **kwargs):\n",
        "    \"\"\"Reward function that gives a bonus for completions in the form of LETTER-DASH-LETTER.\"\"\"\n",
        "    pattern = re.compile(r\"^([A-Z]-)+[A-Z]\")  # Pattern for LETTER-DASH-LETTER\n",
        "\n",
        "    words = [w for w in word]\n",
        "\n",
        "    # Normalize the completions, taking the first line and removing extra whitespace\n",
        "    completion_strings = [\n",
        "        completion.split(\"\\n\")[0].strip() for completion in completions\n",
        "    ]\n",
        "\n",
        "    # Create a list of rewards corresponding to completions\n",
        "    # Each completion that matches the pattern should receive 1.0,\n",
        "    # else 0.0\n",
        "    # rewards = [\n",
        "    #     **********\n",
        "    # ]\n",
        "\n",
        "    # <<< START COMPLETION SECTION\n",
        "    rewards = [\n",
        "        1.0 if pattern.match(c) else 0.0 for w, c in zip(words, completion_strings)\n",
        "    ]\n",
        "    # >>> END COMPLETION SECTION\n",
        "\n",
        "    print(\n",
        "        f\"Letter-dash-letter rewards mean and std: {np.mean(rewards):.3f} +/- {np.std(rewards):.3f}\"\n",
        "    )\n",
        "    return rewards\n",
        "\n",
        "\n",
        "assert reward_response_in_form_of_letter_dash_letter(\n",
        "    completions=[\n",
        "        \"H-E-L-L-O\",\n",
        "        \"hello\",\n",
        "        \"Hi!\",\n",
        "    ],\n",
        "    word=[\n",
        "        \"hello\",\n",
        "        \"hello\",\n",
        "        \"hello\",\n",
        "    ],\n",
        ") == [1, 0, 0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f8a9d1e3",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Student task: Set the GRPOConfig and initialize the trainer\n",
        "# See: https://huggingface.co/docs/trl/main/en/grpo_trainer\n",
        "# TODO: Complete the sections with **********\n",
        "\n",
        "from trl import GRPOConfig, GRPOTrainer\n",
        "\n",
        "training_args = GRPOConfig(\n",
        "    output_dir=\"data/spelling-grpo\",\n",
        "    max_completion_length=30,\n",
        "    logging_steps=5,\n",
        "    # learning_rate=**********,\n",
        "    # num_train_epochs=**********,\n",
        "    # per_device_train_batch_size=**********,\n",
        "    # num_generations=**********,\n",
        "    # lr_scheduler_type=**********,\n",
        "    # beta=**********,\n",
        ")\n",
        "trainer = GRPOTrainer(\n",
        "    model=model,\n",
        "    # Add the parameter for the reward functions\n",
        "    # **********\n",
        "    args=training_args,\n",
        "    train_dataset=ds[\"train\"],\n",
        ")\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "30d14200",
      "metadata": {},
      "source": [
        "Now we define the `SFTTrainer` and run the fine-tuning process."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e30c443c",
      "metadata": {},
      "source": [
        "## Step 5. Evaluate the fine-tuned model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f806e1d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate the fine-tuned model on the same training examples\n",
        "# No changes needed in this cell\n",
        "\n",
        "proportion_correct = 0.0\n",
        "\n",
        "for example in ds[\"train\"].select(range(20)):\n",
        "    prompt = example[\"prompt\"]\n",
        "    completion = example[\"completion\"]\n",
        "    result = check_spelling(\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        prompt=prompt,\n",
        "        actual_spelling=completion,\n",
        "        max_new_tokens=20,\n",
        "    )\n",
        "    proportion_correct += result\n",
        "\n",
        "print(f\"{proportion_correct}/20.0 words correct\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3bbfe48f",
      "metadata": {},
      "source": [
        "The model now performs better on the training data it has seen. But has it generalized? Let's check its performance on the unseen test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af0bab9d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate the fine-tuned model on the unseen test set\n",
        "# No changes needed in this cell\n",
        "\n",
        "proportion_correct = 0.0\n",
        "num_examples = len(ds[\"test\"])\n",
        "\n",
        "for example in ds[\"test\"]:\n",
        "    prompt = example[\"prompt\"]\n",
        "    completion = example[\"completion\"]\n",
        "    result = check_spelling(\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        prompt=prompt,\n",
        "        actual_spelling=completion,\n",
        "        max_new_tokens=20,\n",
        "    )\n",
        "    proportion_correct += result\n",
        "\n",
        "print(f\"{proportion_correct}/{num_examples}.0 words correct\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7b02ba61",
      "metadata": {},
      "source": [
        "It looks like it has improved! Perhaps with a larger dataset and more training, it could get even better."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ed16c690",
      "metadata": {},
      "source": [
        "## Congratulations for completing the exercise! 🎉\n",
        "\n",
        "✅ You did it! You successfully fine-tuned a small language model using PEFT with LoRA to teach it a new skill: spelling! You saw how the base model failed completely at the task, and with a very small amount of data and a short training run, the model started to learn how to spell."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7f093a0b",
      "metadata": {},
      "source": [
        "<br /><br /><br /><br /><br /><br /><br /><br /><br />"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}