# Reinforcement Fine-Tuning on Foundation Models

This module covers advanced fine-tuning techniques using reinforcement learning, specifically Reinforcement Learning from Human Feedback (RLHF) and Group Relative Policy Optimization (GRPO). You will learn to align language models with human preferences and optimize them for specific objectives using reward models and policy optimization.

## Folder Structure

```
> tree
.
|-- README.md
`-- exercises
    |-- README.md
    |-- solution
    |   `-- README.md
    `-- starter
        `-- README.md

4 directories, 4 files
```

## Directory Map
- `exercises/README.md` – overview of reinforcement learning concepts and GRPO methodology
- `exercises/starter/` – GRPO implementation starter code and reward model framework
- `exercises/solution/` – complete reinforcement fine-tuning pipeline with GRPO and evaluation metrics

## Additional Resources
- [GRPO Paper](https://arxiv.org/abs/2402.03300) - Group Relative Policy Optimization research
- [TRL Library Documentation](https://huggingface.co/docs/trl/) - Reinforcement learning with transformers
- [InstructGPT Paper](https://arxiv.org/abs/2203.02155) - Training language models to follow instructions with human feedback
- [Reinforcement Learning: An Introduction](http://incompleteideas.net/book/the-book-2nd.html) - Comprehensive RL textbook

