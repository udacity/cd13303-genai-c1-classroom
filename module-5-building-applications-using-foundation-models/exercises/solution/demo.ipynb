{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo: Build a Sentiment Classifier\n",
    "\n",
    "This notebook is a simplified demonstration of how to use a foundation model to classify text. We will build a simple **sentiment classifier** for movie reviews.\n",
    "\n",
    "The steps are:\n",
    "1.  **Prepare Data**: Create a small, sample dataset of movie reviews.\n",
    "2.  **Build a Basic Classifier**: Use a \"zero-shot\" prompt to classify the reviews.\n",
    "3.  **Build an Improved Classifier**: Use a \"few-shot\" prompt to improve the model's accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected vocareum API key. Using Vocareum OpenAI API base URL.\n"
     ]
    }
   ],
   "source": [
    "import litellm\n",
    "import os\n",
    "\n",
    "if os.getenv(\"OPENAI_API_KEY\"):\n",
    "    litellm.openai_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# If using Vocareum, you can also set your API key here directly\n",
    "# Uncomment and replace the string with your actual Vocareum API key\n",
    "# litellm.openai_key = \"voc-**********\"\n",
    "\n",
    "if (litellm.openai_key or \"\").startswith(\"voc-\"):\n",
    "    litellm.api_base = \"https://openai.vocareum.com/v1\"\n",
    "    print(\"Detected vocareum API key. Using Vocareum OpenAI API base URL.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Prepare the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll create a small, hard-coded dataset of movie reviews. Each review has a piece of text (`review`) and a numeric label (`label`), where `1` is for a positive review and `0` is for a negative one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset created with 5 reviews.\n"
     ]
    }
   ],
   "source": [
    "# A tiny, sample dataset of movie reviews\n",
    "demo_dataset = [\n",
    "    {\"review\": \"An absolute masterpiece, the best film of the year!\", \"label\": 1},  # 0\n",
    "    {\n",
    "        \"review\": \"I was bored from start to finish. A total waste of time.\",\n",
    "        \"label\": 0,\n",
    "    },  # 1\n",
    "    {\n",
    "        \"review\": \"The acting was incredible and the story was so moving.\",\n",
    "        \"label\": 1,\n",
    "    },  # 2\n",
    "    {\"review\": \"While not perfect, it had some good moments.\", \"label\": 1},  # 3\n",
    "    {\"review\": \"A confusing plot and terrible dialogue.\", \"label\": 0},  # 4\n",
    "]\n",
    "\n",
    "print(f\"Dataset created with {len(demo_dataset)} reviews.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The numeric labels aren't very descriptive. Let's map them to human-readable strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label=POSITIVE, review=An absolute masterpiece, the best film of the year!\n",
      "label=NEGATIVE, review=I was bored from start to finish. A total waste of time.\n"
     ]
    }
   ],
   "source": [
    "# Map numeric labels to human-readable labels\n",
    "id2label = {0: \"NEGATIVE\", 1: \"POSITIVE\"}\n",
    "\n",
    "# Print the first two entries to see the new labels\n",
    "for i in range(2):\n",
    "    review = demo_dataset[i][\"review\"]\n",
    "    label_id = demo_dataset[i][\"label\"]\n",
    "    print(f\"label={id2label[label_id]}, review={review}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Build and Evaluate a Basic Classifier (Zero-Shot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's ask the foundation model to classify our reviews. We'll start with a **zero-shot** prompt, which means we will simply tell the model what to do without giving it any examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SYSTEM PROMPT:\n",
      "You are a helpful assistant that classifies movie reviews as POSITIVE or NEGATIVE.\n",
      "Respond only with a JSON object where the keys are the review numbers and the values are the classification.\n",
      "\n",
      "USER PROMPT:\n",
      "0 -> An absolute masterpiece, the best film of the year!\n",
      "1 -> I was bored from start to finish. A total waste of time.\n",
      "2 -> The acting was incredible and the story was so moving.\n",
      "3 -> While not perfect, it had some good moments.\n",
      "4 -> A confusing plot and terrible dialogue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# First, format our reviews into a single string for the prompt\n",
    "reviews_string = \"\"\n",
    "for i, entry in enumerate(demo_dataset):\n",
    "    reviews_string += f\"{i} -> {entry['review']}\\n\"\n",
    "\n",
    "# Define the prompts\n",
    "SYSTEM_PROMPT = \"\"\"You are a helpful assistant that classifies movie reviews as POSITIVE or NEGATIVE.\n",
    "Respond only with a JSON object where the keys are the review numbers and the values are the classification.\"\"\"\n",
    "\n",
    "USER_PROMPT = reviews_string\n",
    "\n",
    "print(\"SYSTEM PROMPT:\")\n",
    "print(SYSTEM_PROMPT)\n",
    "print(\"\\nUSER PROMPT:\")\n",
    "print(USER_PROMPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MODEL RESPONSE:\n",
      "{\n",
      "  \"0\": \"POSITIVE\",\n",
      "  \"1\": \"NEGATIVE\",\n",
      "  \"2\": \"POSITIVE\",\n",
      "  \"3\": \"POSITIVE\",\n",
      "  \"4\": \"NEGATIVE\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from litellm import completion\n",
    "\n",
    "resp = completion(\n",
    "    model=\"gpt-5-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": USER_PROMPT},\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(\"\\nMODEL RESPONSE:\")\n",
    "print(resp.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's paste the response here\n",
    "response_1 = {\n",
    "    \"0\": \"POSITIVE\",\n",
    "    \"1\": \"NEGATIVE\",\n",
    "    \"2\": \"POSITIVE\",\n",
    "    \"3\": \"POSITIVE\",\n",
    "    \"4\": \"NEGATIVE\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's check the accuracy of this response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 100.0\n"
     ]
    }
   ],
   "source": [
    "def get_accuracy(response, dataset):\n",
    "    correct = 0\n",
    "    total = len(response)\n",
    "\n",
    "    for entry_number, prediction in response.items():\n",
    "        entry_number = int(entry_number)\n",
    "        actual_label_id = dataset[entry_number][\"label\"]\n",
    "        actual_label = id2label[actual_label_id]\n",
    "\n",
    "        if prediction.lower() == actual_label.lower():\n",
    "            correct += 1\n",
    "        else:\n",
    "            print(\n",
    "                f\"Mismatch for entry {entry_number}: predicted={prediction}, actual={actual_label}\"\n",
    "            )\n",
    "\n",
    "    accuracy = correct / total * 100\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "accuracy_1 = get_accuracy(response_1, demo_dataset)\n",
    "print(f\"Accuracy: {accuracy_1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's great! Sometimes our model will need additional help to get the right answer. Let's try a few-shot prompt next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Build an Improved Classifier (Few-Shot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try a **few-shot** prompt. We will provide a couple of correctly labeled examples *within the prompt itself* to guide the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SYSTEM PROMPT:\n",
      "You are a helpful assistant that classifies movie reviews as POSITIVE or NEGATIVE.\n",
      "Respond only with a JSON object where the keys are the review numbers and the values are the classification.\n",
      "Here are some examples:\n",
      "\n",
      "EXAMPLE INPUT:\n",
      "0 -> An absolute masterpiece, the best film of the year!\n",
      "1 -> I was bored from start to finish. A total waste of time.\n",
      "\n",
      "EXAMPLE OUTPUT:\n",
      "{\n",
      "  \"0\": \"POSITIVE\",\n",
      "  \"1\": \"NEGATIVE\"\n",
      "}\n",
      "\n",
      "\n",
      "USER PROMPT:\n",
      "0 -> An absolute masterpiece, the best film of the year!\n",
      "1 -> I was bored from start to finish. A total waste of time.\n",
      "2 -> The acting was incredible and the story was so moving.\n",
      "3 -> While not perfect, it had some good moments.\n",
      "4 -> A confusing plot and terrible dialogue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a string with the first two reviews as examples\n",
    "example_string = \"\"\"\n",
    "Here are some examples:\n",
    "\n",
    "EXAMPLE INPUT:\n",
    "0 -> An absolute masterpiece, the best film of the year!\n",
    "1 -> I was bored from start to finish. A total waste of time.\n",
    "\n",
    "EXAMPLE OUTPUT:\n",
    "{\n",
    "  \"0\": \"POSITIVE\",\n",
    "  \"1\": \"NEGATIVE\"\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# The new SYSTEM_PROMPT now includes the examples\n",
    "IMPROVED_SYSTEM_PROMPT = SYSTEM_PROMPT + example_string\n",
    "\n",
    "print(\"SYSTEM PROMPT:\")\n",
    "print(IMPROVED_SYSTEM_PROMPT)\n",
    "print(\"\\nUSER PROMPT:\")\n",
    "print(USER_PROMPT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the added examples, the model can better understand the task and improve its accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MODEL RESPONSE:\n",
      "{\n",
      "  \"0\": \"POSITIVE\",\n",
      "  \"1\": \"NEGATIVE\",\n",
      "  \"2\": \"POSITIVE\",\n",
      "  \"3\": \"POSITIVE\",\n",
      "  \"4\": \"NEGATIVE\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "resp = completion(\n",
    "    model=\"gpt-5-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": IMPROVED_SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": USER_PROMPT},\n",
    "    ],\n",
    ")\n",
    "print(\"\\nMODEL RESPONSE:\")\n",
    "print(resp.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 100.0\n"
     ]
    }
   ],
   "source": [
    "response_2 = {\n",
    "    \"0\": \"POSITIVE\",\n",
    "    \"1\": \"NEGATIVE\",\n",
    "    \"2\": \"POSITIVE\",\n",
    "    \"3\": \"POSITIVE\",\n",
    "    \"4\": \"NEGATIVE\",\n",
    "}\n",
    "\n",
    "accuracy_2 = get_accuracy(response_2, demo_dataset)\n",
    "print(f\"Accuracy: {accuracy_2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "It's important to note the following: **prompt engineering techniques such as guiding the model with examples may improve the accuracy of the model, but to actually show that is true in your specific use case requires larger dataset and more rigorous evaluation than we have done here.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br /><br /><br /><br /><br /><br /><br /><br /><br />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
