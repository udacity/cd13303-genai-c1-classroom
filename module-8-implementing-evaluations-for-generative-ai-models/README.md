# Implementing Evaluations for Generative AI Models

This module focuses on comprehensive evaluation strategies for generative AI systems. You will learn to design robust evaluation frameworks, implement multiple evaluation metrics, and systematically assess model performance across different dimensions of quality and safety.

## Folder Structure

```
> tree
.
|-- README.md
`-- exercises
    |-- README.md
    |-- solution
    |   `-- README.md
    `-- starter
        `-- README.md

4 directories, 4 files
```

## Directory Map
- `exercises/README.md` – overview of evaluation concepts and exercise structure
- `exercises/starter/` – evaluation framework starter code and sample datasets
- `exercises/solution/` – complete evaluation pipeline with multiple metric implementations

## Additional Resources
- [BLEU Score Paper](https://aclanthology.org/P02-1040.pdf) - Original BLEU evaluation metric
- [ROUGE Evaluation](https://aclanthology.org/W04-1013.pdf) - ROUGE metrics for text summarization
- [BERTScore Paper](https://arxiv.org/abs/1904.09675) - Contextual evaluation using BERT embeddings
- [Human Evaluation Guidelines](https://arxiv.org/abs/2107.00061) - Best practices for human evaluation of NLP systems

