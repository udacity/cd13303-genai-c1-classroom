{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "c946b9af",
      "metadata": {},
      "source": [
        "# Exercise: GenAI Evaluation Medley\n",
        "\n",
        "Let's practice exact evaluation, AI-as-judge mechanics, and benchmarking by completing small, focused coding tasks.\n",
        "\n",
        "## Outline\n",
        "\n",
        "We will cover the following evaluation techniques:\n",
        "1. Exact Match: Implement a function to compute the exact match score between predicted and reference answers.\n",
        "2. Lexical Similarity: Calculate ROUGE scores to assess the overlap between predicted and reference texts.\n",
        "3. Semantic Similarity: Use embeddings to compute cosine similarity between predicted and reference texts.\n",
        "4. Functional Correctness: Evaluate code generation by executing predicted code and comparing outputs.\n",
        "5. Pass@K: Implement the Pass@K metric.\n",
        "6. LLM-as-a-Judge or AI-as-a-Judge: Use a language model to evaluate the quality of predictions based on a rubric."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d72f89e6",
      "metadata": {},
      "source": [
        "## Setup\n",
        "Now we import standard libraries used across exercises and set basic configuration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b2778307",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Student Task: Set up the OpenAI API key and base URL from environment variables\n",
        "# TODO: If using Vocareum, set the API key directly in the code below\n",
        "\n",
        "import litellm\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "if os.getenv(\"OPENAI_API_KEY\"):\n",
        "    litellm.openai_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "# If using Vocareum, you can also set your API key here directly\n",
        "# Uncomment and replace the string with your actual Vocareum API key\n",
        "# litellm.openai_key = \"voc-**********\"\n",
        "\n",
        "if (litellm.openai_key or \"\").startswith(\"voc-\"):\n",
        "    litellm.api_base = \"https://openai.vocareum.com/v1\"\n",
        "    print(\"Using Vocareum OpenAI API base URL\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7cc74fd4",
      "metadata": {},
      "source": [
        "## Exact Match (EM)\n",
        "Let's compute exact-match accuracy after simple normalization (lowercase and trim)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "58b84285",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Student task: Implement exact_match and compute EM\n",
        "# TODO: Complete the sections marked with **********\n",
        "\n",
        "preds = [\"Lima\", \"ayacucho\", \"Cusco\", \"Arequipa\"]\n",
        "labels = [\"lima\", \"Ayacucho\", \"Cusco\", \"Trujillo\"]\n",
        "\n",
        "\n",
        "def normalize(s: str) -> str:\n",
        "    \"\"\"Normalize the string by lowercasing and stripping whitespace.\"\"\"\n",
        "    return s.lower().strip()\n",
        "\n",
        "\n",
        "def exact_match(pred: str, label: str) -> int:\n",
        "    # return 1 if normalized strings are identical, else 0\n",
        "    return_value = \"**********\"\n",
        "\n",
        "\n",
        "    return return_value\n",
        "\n",
        "\n",
        "em_scores = [exact_match(p, l) for p, l in zip(preds, labels)]\n",
        "em = sum(em_scores) / len(em_scores)\n",
        "print(\"EM:\", em)\n",
        "\n",
        "assert em == 0.75, (\n",
        "    f\"EM should be 0.75, but got {em}. Please check your exact_match function.\"\n",
        ")\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9c71e252",
      "metadata": {},
      "source": [
        "## Lexical Similarity (ROUGE)\n",
        "\n",
        "Let's compute ROUGE scores using the `evaluate` library.\n",
        "\n",
        "Read more at: https://huggingface.co/spaces/evaluate-metric/rouge/blob/main/README.md"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b039506b",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Student task: Compute ROUGE-L using LCS length\n",
        "# Complete the sections with **********\n",
        "\n",
        "\n",
        "# Define candidate and reference texts\n",
        "pred = \"The capital of Peru is Lima\"\n",
        "label = \"Lima is the capital of Peru\"\n",
        "\n",
        "\n",
        "# Import the evaluate library\n",
        "# **************\n",
        "\n",
        "# Load the ROUGE metric\n",
        "# **************\n",
        "\n",
        "# Compute ROUGE scores\n",
        "# **************\n",
        "\n",
        "\n",
        "assert isinstance(results, dict), (\n",
        "    f\"Results should be a dictionary, but got {type(results)}. See the evaluate library documentation for ROUGE usage.\"\n",
        ")\n",
        "keys = [\"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\"]\n",
        "for key in keys:\n",
        "    assert key in results, (\n",
        "        f\"Missing key '{key}' in results. Expected keys: {keys}. See the evaluate library documentation for ROUGE usage.\"\n",
        "    )\n",
        "\n",
        "results\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "15822ca6",
      "metadata": {},
      "source": [
        "## Semantic Similarity using Cosine Similarity\n",
        "\n",
        "We'll use the `sentence-transformers` library to compute semantic similarity between predicted and reference sentences. The model \"all-MiniLM-L6-v2\" is a lightweight model that can run on GPUs.\n",
        "\n",
        "Read more here: https://sbert.net/docs/quickstart.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d276d34",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# 1. Load a pretrained Sentence Transformer model\n",
        "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "# 2. Some example sentences\n",
        "sentences = [\n",
        "    \"Hi there!\",\n",
        "    \"This is a test sentence.\",\n",
        "]\n",
        "\n",
        "# 3. Generate embeddings\n",
        "embeddings = model.encode(sentences)\n",
        "\n",
        "# 4. Verify we have 2 embeddings of dimension 384 each\n",
        "assert embeddings.shape == (2, 384)\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a78dfbf",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Student task: Write a semantically different prediction sentence and compute embeddings\n",
        "# Complete the sections with **********\n",
        "\n",
        "labels = [\"Cusco is in Peru\", \"Ayacucho is a region\", \"Trujillo beaches are marvelous\"]\n",
        "preds = [\n",
        "    \"Peru includes Cusco\",\n",
        "    \"Ayacucho is a department\",\n",
        "    # Write a sentence that is very semantically different from the prediction\n",
        "    # \"***********\"\n",
        "]\n",
        "\n",
        "\n",
        "# Get the embeddings for each sentence\n",
        "pred_embeddings = \"**********\"\n",
        "label_embeddings = \"**********\"\n",
        "\n",
        "\n",
        "assert pred_embeddings.shape == (3, 384), (\n",
        "    f\"Expected shape (3, 384), got {pred_embeddings.shape}\"\n",
        ")\n",
        "assert label_embeddings.shape == (3, 384), (\n",
        "    f\"Expected shape (3, 384), got {label_embeddings.shape}\"\n",
        ")\n",
        "\n",
        "pred_embeddings, label_embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6e3b346c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate the cosine similarity for each pair of embeddings\n",
        "# No changes needed in this cell, but if it fails, check the above cell\n",
        "\n",
        "cosine_similarity = [\n",
        "    # Cosine similarity for two vectors a and b is defined as:\n",
        "    # cos_sim(a, b) = (a . b) / (||a|| * ||b||)\n",
        "    # where (a . b) is the dot product of a and b,\n",
        "    # and ||a|| and ||b|| are the magnitudes (norms) of vectors a and b respectively.\n",
        "    float(\n",
        "        np.dot(pred_embeddings[i], label_embeddings[i])\n",
        "        / np.linalg.norm(pred_embeddings[i])\n",
        "        / np.linalg.norm(label_embeddings[i])\n",
        "    )\n",
        "    for i in range(len(preds))\n",
        "]\n",
        "\n",
        "# Compute cosine similarity between the two embeddings\n",
        "for i, (p, l, cos_sim) in enumerate(zip(preds, labels, cosine_similarity)):\n",
        "    print(f\"Pair {i + 1}:\")\n",
        "    print(f\"  Pred: {p}\")\n",
        "    print(f\"  Label: {l}\")\n",
        "    print(f\"  Cosine Similarity: {cos_sim:.4f}\\n\")\n",
        "\n",
        "# Check that the last pair has the lowest similarity\n",
        "assert cosine_similarity[-1] < cosine_similarity[0], (\n",
        "    \"The last pair should have the lowest cosine similarity. Please check your prediction sentence.\"\n",
        ")\n",
        "assert cosine_similarity[-1] < cosine_similarity[1], (\n",
        "    \"The last pair should have the lowest cosine similarity. Please check your prediction sentence.\"\n",
        ")\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2bbaf675",
      "metadata": {},
      "source": [
        "## Functional Correctness\n",
        "Let's evaluate code-generation by running a tiny function against unit tests (execution accuracy)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ecc358d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Student task: Complete the evaluation of the sort_and_normalize function\n",
        "# Complete the sections with **********\n",
        "\n",
        "\n",
        "def sort_and_normalize(s: str) -> str:\n",
        "    \"\"\"Sort the words in the string\"\"\"\n",
        "\n",
        "    # Our toy function will fail on this edge case\n",
        "    if \"armadillo\" in s:\n",
        "        s = s.replace(\"armadillo\", \"kitty\")\n",
        "\n",
        "    return \" \".join(sorted(s.split()))\n",
        "\n",
        "\n",
        "preds = [\n",
        "    \"the capybara is the largest rodent\",\n",
        "    \"an armadillo has a hard shell\",\n",
        "    \"elephants are the largest land animals\",\n",
        "]\n",
        "labels = [\n",
        "    \"capybara is largest rodent the the\",\n",
        "    \"a an armadillo hard has shell\",\n",
        "    \"animals are elephants land largest the\",\n",
        "]\n",
        "\n",
        "# Write tests to check if sort_and_normalize works correctly\n",
        "results = [\n",
        "    # \"**********\"\n",
        "    for p, l in zip(preds, labels)\n",
        "]\n",
        "\n",
        "print(\"Proportion of tests passed:\", sum(results) / len(results))\n",
        "\n",
        "assert sum(results) == 2, (\n",
        "    f\"2 tests should pass, but got {sum(results)}. Please check how your are evaluating the results.\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d5b5ab6",
      "metadata": {},
      "source": [
        "## Pass@k\n",
        "\n",
        "Let's simulate multiple samples for a single task and compute pass@k (1 if any sample equals the gold)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e5459f06",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Student task: Implement pass_at_k\n",
        "# Complete the sections with **********\n",
        "\n",
        "label = \"Lima\"\n",
        "samples = [\"Lima\", \"Arequipa\", \"Cusco\", \"Lima\"]\n",
        "\n",
        "\n",
        "# Implement pass_at_k with signature (samples: List[str], label: str) -> int\n",
        "# **********\n",
        "\n",
        "\n",
        "\n",
        "print(\"pass@4 =\", pass_at_k(samples, label))\n",
        "\n",
        "assert pass_at_k(samples, label) == 1, (\n",
        "    f\"pass@4 should be 1, but got {pass_at_k(samples, label)}. Please check your pass_at_k function.\"\n",
        ")\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "59b336e1",
      "metadata": {},
      "source": [
        "## LLM as a Judge\n",
        "\n",
        "Let's create a function that calls an LLM to compare predicted values and reference values (if applicable) and return a score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97697d6b",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Student task: Complete the LLM-as-a-judge function\n",
        "# Complete the sections with **********\n",
        "\n",
        "\n",
        "def llm_as_judge(pred: str, rubric: str, label: str | None = None) -> float:\n",
        "    \"\"\"Use an LLM to judge the quality of a prediction against a rubric and optional label.\"\"\"\n",
        "    from litellm import completion\n",
        "\n",
        "    # Write a system prompt that instructs the LLM to use the rubric to score the prediction\n",
        "    # The response should be formatted as:\n",
        "    # <reasoning>...</reasoning>\n",
        "    # <score>FLOAT_ANSWER</score>\n",
        "    # where FLOAT_ANSWER is a float between 0 and 1.\n",
        "    # We will extract FLOAT_ANSWER from the response later\n",
        "\n",
        "    SYSTEM_PROMPT = \"**********\"\n",
        "\n",
        "\n",
        "    # Create a user prompt with the prediction and, optionally, the label\n",
        "    # **********\n",
        "\n",
        "\n",
        "    # Call the LLM using litellm with the system and user prompts (use the model gpt-5-nano)\n",
        "    # See: https://github.com/BerriAI/litellm\n",
        "\n",
        "    # response = completion(**********)\n",
        "\n",
        "\n",
        "    text_response = response[\"choices\"][0][\"message\"][\"content\"]\n",
        "    print(\"LLM response:\", text_response)\n",
        "\n",
        "    # Extract FLOAT_ANSWER from the response\n",
        "\n",
        "    # float_answer = **********\n",
        "\n",
        "\n",
        "    return float_answer\n",
        "\n",
        "\n",
        "# Write a rubric for evaluating if the prediction is the capital of the label country\n",
        "# 1.0 if correct, 0.5 if a city in the same country, 0.0 otherwise\n",
        "\n",
        "# **********\n",
        "\n",
        "\n",
        "assert (\n",
        "    llm_as_judge(\n",
        "        pred=\"Manila\",\n",
        "        label=\"Philippines\",\n",
        "        rubric=RUBRIC,\n",
        "    )\n",
        "    == 1.0\n",
        "), \"Manila is the capital of the Philippines\"\n",
        "\n",
        "assert (\n",
        "    llm_as_judge(\n",
        "        pred=\"Cebu\",\n",
        "        label=\"Philippines\",\n",
        "        rubric=RUBRIC,\n",
        "    )\n",
        "    == 0.5\n",
        "), \"Cebu is a city in the Philippines, but not the capital\"\n",
        "\n",
        "assert (\n",
        "    llm_as_judge(\n",
        "        pred=\"Tokyo\",\n",
        "        label=\"Philippines\",\n",
        "        rubric=RUBRIC,\n",
        "    )\n",
        "    == 0.0\n",
        "), \"Tokyo is not in the Philippines\"\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa792f32",
      "metadata": {},
      "source": [
        "Congrats! You have completed the evaluation exercise. Proper evaluation is the bedrock for building reliable AI systems. Great job! üëèüëèüëè"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b4089a86",
      "metadata": {},
      "source": [
        "<br /><br /><br /><br /><br /><br /><br /><br /><br />"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}