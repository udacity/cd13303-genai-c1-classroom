{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GenAI Evaluation: A Quick Demo\n",
    "\n",
    "Welcome! This notebook is a brief demonstration of several common techniques used to evaluate the output of Generative AI models. We'll walk through simple, clear examples for each method. Pay close attention to how each technique measures a different aspect of quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected vocareum API key. Using Vocareum OpenAI API base URL.\n"
     ]
    }
   ],
   "source": [
    "import litellm\n",
    "import os\n",
    "\n",
    "if os.getenv(\"OPENAI_API_KEY\"):\n",
    "    litellm.openai_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# If using Vocareum, you can also set your API key here directly\n",
    "# Uncomment and replace the string with your actual Vocareum API key\n",
    "# litellm.openai_key = \"voc-**********\"\n",
    "\n",
    "if (litellm.openai_key or \"\").startswith(\"voc-\"):\n",
    "    litellm.api_base = \"https://openai.vocareum.com/v1\"\n",
    "    print(\"Detected vocareum API key. Using Vocareum OpenAI API base URL.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup: Imports and Configuration\n",
    "\n",
    "First, we import the necessary libraries. This cell brings in tools for data handling, math, and specific evaluation metrics from libraries like `scikit-learn` and `evaluate`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, random, re, sys, platform, time, json\n",
    "from typing import List, Dict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from evaluate import load\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Set a seed for reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Exact Match (EM)\n",
    "\n",
    "**Exact Match** is the simplest evaluation metric. It checks if the model's output is identical to the reference answer. We often apply a simple normalization step (like lowercasing) first.\n",
    "\n",
    "It's useful for tasks with a single, clear correct answer, like multiple-choice questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Individual Scores: [1, 1, 0]\n",
      "Average Exact Match Accuracy: 0.67\n"
     ]
    }
   ],
   "source": [
    "# Let's compare predicted fruit names with the correct labels.\n",
    "preds = [\"Apple\", \"banana \", \" Orange\"]\n",
    "labels = [\"apple\", \"banana\", \"grape\"]\n",
    "\n",
    "\n",
    "def normalize(s: str) -> str:\n",
    "    \"\"\"Normalize a string by lowercasing and stripping whitespace.\"\"\"\n",
    "    return s.lower().strip()\n",
    "\n",
    "\n",
    "def exact_match(pred: str, label: str) -> int:\n",
    "    \"\"\"Return 1 if normalized strings are identical, else 0.\"\"\"\n",
    "    return int(normalize(pred) == normalize(label))\n",
    "\n",
    "\n",
    "# Calculate EM score for each pair\n",
    "em_scores = [exact_match(p, l) for p, l in zip(preds, labels)]\n",
    "\n",
    "# The final score is the average of individual scores\n",
    "em_accuracy = sum(em_scores) / len(em_scores)\n",
    "\n",
    "print(f\"Individual Scores: {em_scores}\")\n",
    "print(f\"Average Exact Match Accuracy: {em_accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Lexical Similarity (ROUGE)\n",
    "\n",
    "When answers can be phrased differently, Exact Match is too strict. **ROUGE (Recall-Oriented Understudy for Gisting Evaluation)** measures the overlap of words (n-grams) between the prediction and the label.\n",
    "\n",
    "It's commonly used for evaluating text summarization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE-1 Score: 0.8000\n",
      "ROUGE-L Score: 0.6000\n"
     ]
    }
   ],
   "source": [
    "# Define a prediction and a reference text\n",
    "pred = \"the quick brown fox\"\n",
    "label = \"the fox is quick and brown\"\n",
    "\n",
    "# Load the ROUGE metric from the 'evaluate' library\n",
    "rouge = load(\"rouge\")\n",
    "\n",
    "# Compute the scores\n",
    "results = rouge.compute(predictions=[pred], references=[label])\n",
    "\n",
    "# ROUGE-1 measures unigram (single word) overlap.\n",
    "# ROUGE-L measures the longest common subsequence.\n",
    "print(f\"ROUGE-1 Score: {results['rouge1']:.4f}\")\n",
    "print(f\"ROUGE-L Score: {results['rougeL']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Semantic Similarity\n",
    "\n",
    "What if two sentences mean the same thing but use different words? **Semantic Similarity** addresses this. We convert sentences into numerical vectors (embeddings) and measure the cosine similarity between them. A score near 1.0 means very similar in meaning, while a score near 0.0 means very different.\n",
    "\n",
    "This is great for evaluating paraphrasing or nuanced answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pair 1:\n",
      "  Pred:  'Dogs make great companions'\n",
      "  Label: 'A dog is a loyal pet'\n",
      "  Similarity: 0.6147\n",
      "\n",
      "Pair 2:\n",
      "  Pred:  'A cat is a solitary creature'\n",
      "  Label: 'Cats are independent animals'\n",
      "  Similarity: 0.6848\n",
      "\n",
      "Pair 3:\n",
      "  Pred:  'The ocean is vast'\n",
      "  Label: 'The sky is blue'\n",
      "  Similarity: 0.3098\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. Load a pretrained Sentence Transformer model\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# 2. Define prediction and label sentences\n",
    "labels = [\"A dog is a loyal pet\", \"Cats are independent animals\", \"The sky is blue\"]\n",
    "preds = [\n",
    "    \"Dogs make great companions\",\n",
    "    \"A cat is a solitary creature\",\n",
    "    \"The ocean is vast\",\n",
    "]\n",
    "\n",
    "# 3. Generate embeddings for each list\n",
    "pred_embeddings = model.encode(preds)\n",
    "label_embeddings = model.encode(labels)\n",
    "\n",
    "# 4. Calculate cosine similarity for each pair\n",
    "for i in range(len(preds)):\n",
    "    # The formula is: (A dot B) / (||A|| * ||B||)\n",
    "    similarity = np.dot(pred_embeddings[i], label_embeddings[i]) / (\n",
    "        np.linalg.norm(pred_embeddings[i]) * np.linalg.norm(label_embeddings[i])\n",
    "    )\n",
    "    print(\n",
    "        f\"Pair {i + 1}:\\n  Pred:  '{preds[i]}'\\n  Label: '{labels[i]}'\\n  Similarity: {similarity:.4f}\\n\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Functional Correctness\n",
    "\n",
    "For code generation, we need to know if the code actually works. **Functional Correctness** can evaluate this by running the generated code against a set of unit tests. The score is the proportion of tests that pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 'hello' -> Output: 'OLLEH', Expected: 'OLLEH'\n",
      "Input: 'world1' -> Output: 'ERROR - CONTAINS DIGITS', Expected: '1DLROW'\n",
      "Input: 'python' -> Output: 'NOHTYP', Expected: 'NOHTYP'\n",
      "\n",
      "Proportion of tests passed: 0.67\n"
     ]
    }
   ],
   "source": [
    "# This function is supposed to reverse and capitalize a string,\n",
    "# but it has a bug: it fails if the string contains a number.\n",
    "def reverse_and_capitalize(s: str) -> str:\n",
    "    \"\"\"Reverse and capitalize a string, with a hidden bug.\"\"\"\n",
    "    if any(char.isdigit() for char in s):\n",
    "        return \"ERROR - CONTAINS DIGITS\"\n",
    "    return s[::-1].upper()\n",
    "\n",
    "\n",
    "# Test cases: one prediction will trigger the bug\n",
    "code_preds = [\"hello\", \"world1\", \"python\"]\n",
    "test_labels = [\"OLLEH\", \"1DLROW\", \"NOHTYP\"]\n",
    "\n",
    "# Run the generated code against the test labels\n",
    "results = []\n",
    "for pred_code, label in zip(code_preds, test_labels):\n",
    "    output = reverse_and_capitalize(pred_code)\n",
    "    print(f\"Input: '{pred_code}' -> Output: '{output}', Expected: '{label}'\")\n",
    "    results.append(output == label)\n",
    "\n",
    "pass_rate = sum(results) / len(results)\n",
    "print(f\"\\nProportion of tests passed: {pass_rate:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Pass@k\n",
    "\n",
    "Sometimes we ask a model to generate multiple (`k`) possible answers for one problem. The **Pass@k** metric measures if at least one of these `k` attempts is correct. If any sample is correct, the entire set is considered a \"pass\" (score of 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples: ['red', 'yellow', 'green', 'blue']\n",
      "Label: blue\n",
      "Pass@4 Score: 1\n"
     ]
    }
   ],
   "source": [
    "def pass_at_k(samples: List[str], label: str) -> int:\n",
    "    \"\"\"Return 1 if any sample in the list matches the label, else 0.\"\"\"\n",
    "    return int(any(s == label for s in samples))\n",
    "\n",
    "\n",
    "# The model generated 4 possible answers for \"Name a primary color.\"\n",
    "label = \"blue\"\n",
    "samples = [\"red\", \"yellow\", \"green\", \"blue\"]\n",
    "\n",
    "# Check if any of the 4 samples is correct\n",
    "pass_score = pass_at_k(samples, label)\n",
    "\n",
    "print(f\"Samples: {samples}\")\n",
    "print(f\"Label: {label}\")\n",
    "print(f\"Pass@4 Score: {pass_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. LLM-as-a-Judge\n",
    "\n",
    "For complex, subjective tasks (like creativity or helpfulness), we can use another powerful LLM to act as a judge. We provide the judge with the prediction, the reference (if any), and a detailed **rubric**. The judge then provides a score and reasoning.\n",
    "\n",
    "**Note**: To make this demo run without an API key, we will *simulate* the LLM judge's behavior with a mock function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Judge Response:\n",
      "Reason: The prediction \"Lion\" exactly matches the label \"Lion,\" so it meets the criterion for a full match.\n",
      "\n",
      "FINAL SCORE: 1.0\n",
      "\n",
      "--> Final Score: 1.0\n",
      "\n",
      "LLM Judge Response:\n",
      "Reason: Tiger and lion are different animals, but both are mammals (both big cats), so they fall under the \"different animal but same biological class\" case.\n",
      "\n",
      "FINAL SCORE: 0.5\n",
      "\n",
      "--> Final Score: 0.5\n",
      "\n",
      "LLM Judge Response:\n",
      "The prediction \"Snake\" is a reptile while the label \"Lion\" is a mammal, so they are different biological classes and do not match the same animal — score 0.0.\n",
      "\n",
      "FINAL SCORE: 0.0\n",
      "\n",
      "--> Final Score: 0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This is our rubric for the judge.\n",
    "RUBRIC = \"\"\"\n",
    "Score 1.0 if the predicted animal is the same as the label.\n",
    "Score 0.5 if the prediction is a different animal but from the same biological class (e.g., both are mammals).\n",
    "Score 0.0 otherwise (e.g., a mammal and a reptile).\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# A MOCK function to simulate an LLM judge's response.\n",
    "# It returns a score based on a pre-defined logic that follows the rubric.\n",
    "def llm_as_judge(pred: str, label: str, rubric: str) -> float:\n",
    "    \"\"\"A simulated LLM judge that scores animal predictions.\"\"\"\n",
    "    from litellm import completion\n",
    "    import re\n",
    "\n",
    "    resp = completion(\n",
    "        model=\"gpt-5-mini\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": (\n",
    "                    \"You are an expert judge that scores answers based on a given rubric. \"\n",
    "                    \"You respond with a reason, and then FINAL SCORE: <score>.\"\n",
    "                ),\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": (\n",
    "                    f\"Rubric:\\n{rubric}\\n\\nPrediction: {pred}\\nLabel: {label}\\n\\nWhat score should be assigned\"\n",
    "                    \" based on the rubric? Respond with scores in the rubric.\"\n",
    "                ),\n",
    "            },\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    resp_text = resp.choices[0].message.content\n",
    "    print(f\"LLM Judge Response:\\n{resp_text}\\n\")\n",
    "\n",
    "    pattern = r\"FINAL SCORE:\\s*([0-9]*\\.?[0-9]+)\"\n",
    "    match = re.search(pattern, resp_text)\n",
    "    if match:\n",
    "        score = float(match.group(1))\n",
    "        return score\n",
    "    else:\n",
    "        print(\n",
    "            \"Warning: Could not find FINAL SCORE in the response. Defaulting to None.\"\n",
    "        )\n",
    "        return None\n",
    "\n",
    "\n",
    "# --- Test Case 1: Perfect Match ---\n",
    "score1 = llm_as_judge(pred=\"Lion\", label=\"Lion\", rubric=RUBRIC)\n",
    "print(f\"--> Final Score: {score1}\\n\")\n",
    "\n",
    "# --- Test Case 2: Same Class ---\n",
    "score2 = llm_as_judge(pred=\"Tiger\", label=\"Lion\", rubric=RUBRIC)\n",
    "print(f\"--> Final Score: {score2}\\n\")\n",
    "\n",
    "# --- Test Case 3: Different Class ---\n",
    "score3 = llm_as_judge(pred=\"Snake\", label=\"Lion\", rubric=RUBRIC)\n",
    "print(f\"--> Final Score: {score3}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "That's a quick tour of several key evaluation methods! Each one has its strengths and is suited for different types of tasks. Choosing the right metric is crucial for understanding your model's performance and making improvements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br /><br /><br /><br /><br /><br /><br /><br /><br />"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
