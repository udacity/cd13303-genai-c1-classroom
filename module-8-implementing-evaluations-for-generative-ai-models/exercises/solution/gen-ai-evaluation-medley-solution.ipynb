{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c946b9af",
   "metadata": {},
   "source": [
    "# Exercise: GenAI Evaluation Medley\n",
    "\n",
    "Let's practice exact evaluation, AI-as-judge mechanics, and benchmarking by completing small, focused coding tasks.\n",
    "\n",
    "## Outline\n",
    "\n",
    "We will cover the following evaluation techniques:\n",
    "1. Exact Match: Implement a function to compute the exact match score between predicted and reference answers.\n",
    "2. Lexical Similarity: Calculate ROUGE scores to assess the overlap between predicted and reference texts.\n",
    "3. Semantic Similarity: Use embeddings to compute cosine similarity between predicted and reference texts.\n",
    "4. Functional Correctness: Evaluate code generation by executing predicted code and comparing outputs.\n",
    "5. Pass@K: Implement the Pass@K metric.\n",
    "6. LLM-as-a-Judge or AI-as-a-Judge: Use a language model to evaluate the quality of predictions based on a rubric."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72f89e6",
   "metadata": {},
   "source": [
    "## Setup\n",
    "Now we import standard libraries used across exercises and set basic configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2778307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Vocareum OpenAI API base URL\n"
     ]
    }
   ],
   "source": [
    "# Student Task: Set up the OpenAI API key and base URL from environment variables\n",
    "# TODO: If using Vocareum, set the API key directly in the code below\n",
    "\n",
    "import litellm\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "if os.getenv(\"OPENAI_API_KEY\"):\n",
    "    litellm.openai_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# If using Vocareum, you can also set your API key here directly\n",
    "# Uncomment and replace the string with your actual Vocareum API key\n",
    "# litellm.openai_key = \"voc-**********\"\n",
    "\n",
    "if (litellm.openai_key or \"\").startswith(\"voc-\"):\n",
    "    litellm.api_base = \"https://openai.vocareum.com/v1\"\n",
    "    print(\"Using Vocareum OpenAI API base URL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc74fd4",
   "metadata": {},
   "source": [
    "## Exact Match (EM)\n",
    "Let's compute exact-match accuracy after simple normalization (lowercase and trim)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "58b84285",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EM: 0.75\n"
     ]
    }
   ],
   "source": [
    "# Student task: Implement exact_match and compute EM\n",
    "# TODO: Complete the sections marked with **********\n",
    "\n",
    "preds = [\"Lima\", \"ayacucho\", \"Cusco\", \"Arequipa\"]\n",
    "labels = [\"lima\", \"Ayacucho\", \"Cusco\", \"Trujillo\"]\n",
    "\n",
    "\n",
    "def normalize(s: str) -> str:\n",
    "    \"\"\"Normalize the string by lowercasing and stripping whitespace.\"\"\"\n",
    "    return s.lower().strip()\n",
    "\n",
    "\n",
    "def exact_match(pred: str, label: str) -> int:\n",
    "    # return 1 if normalized strings are identical, else 0\n",
    "    return_value = \"**********\"\n",
    "\n",
    "    # <<< START SOLUTION SECTION\n",
    "    return_value = int(normalize(pred) == normalize(label))\n",
    "    # >>> END SOLUTION SECTION\n",
    "\n",
    "    return return_value\n",
    "\n",
    "\n",
    "em_scores = [exact_match(p, l) for p, l in zip(preds, labels)]\n",
    "em = sum(em_scores) / len(em_scores)\n",
    "print(\"EM:\", em)\n",
    "\n",
    "assert em == 0.75, (\n",
    "    f\"EM should be 0.75, but got {em}. Please check your exact_match function.\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c71e252",
   "metadata": {},
   "source": [
    "## Lexical Similarity (ROUGE)\n",
    "\n",
    "Let's compute ROUGE scores using the `evaluate` library.\n",
    "\n",
    "Read more at: https://huggingface.co/spaces/evaluate-metric/rouge/blob/main/README.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "b039506b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge1': np.float64(1.0),\n",
       " 'rouge2': np.float64(0.6),\n",
       " 'rougeL': np.float64(0.6666666666666666),\n",
       " 'rougeLsum': np.float64(0.6666666666666666)}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Student task: Compute ROUGE-L using LCS length\n",
    "# Complete the sections with **********\n",
    "\n",
    "\n",
    "# Define candidate and reference texts\n",
    "pred = \"The capital of Peru is Lima\"\n",
    "label = \"Lima is the capital of Peru\"\n",
    "\n",
    "\n",
    "# Import the evaluate library\n",
    "# **************\n",
    "\n",
    "# Load the ROUGE metric\n",
    "# **************\n",
    "\n",
    "# Compute ROUGE scores\n",
    "# **************\n",
    "\n",
    "# <<< START SOLUTION SECTION\n",
    "\n",
    "# Import the evaluate library\n",
    "from evaluate import load\n",
    "\n",
    "# Load the ROUGE metric\n",
    "rouge = load(\"rouge\")\n",
    "\n",
    "# Compute ROUGE scores\n",
    "results = rouge.compute(predictions=[pred], references=[label])\n",
    "\n",
    "\n",
    "# >>> END SOLUTION SECTION\n",
    "\n",
    "assert isinstance(results, dict), (\n",
    "    f\"Results should be a dictionary, but got {type(results)}. See the evaluate library documentation for ROUGE usage.\"\n",
    ")\n",
    "keys = [\"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\"]\n",
    "for key in keys:\n",
    "    assert key in results, (\n",
    "        f\"Missing key '{key}' in results. Expected keys: {keys}. See the evaluate library documentation for ROUGE usage.\"\n",
    "    )\n",
    "\n",
    "results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15822ca6",
   "metadata": {},
   "source": [
    "## Semantic Similarity using Cosine Similarity\n",
    "\n",
    "We'll use the `sentence-transformers` library to compute semantic similarity between predicted and reference sentences. The model \"all-MiniLM-L6-v2\" is a lightweight model that can run on GPUs.\n",
    "\n",
    "Read more here: https://sbert.net/docs/quickstart.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "5d276d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# 1. Load a pretrained Sentence Transformer model\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# 2. Some example sentences\n",
    "sentences = [\n",
    "    \"Hi there!\",\n",
    "    \"This is a test sentence.\",\n",
    "]\n",
    "\n",
    "# 3. Generate embeddings\n",
    "embeddings = model.encode(sentences)\n",
    "\n",
    "# 4. Verify we have 2 embeddings of dimension 384 each\n",
    "assert embeddings.shape == (2, 384)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "9a78dfbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.06145667, -0.06237286, -0.03735719, ..., -0.00960599,\n",
       "          0.03519065, -0.01402609],\n",
       "        [-0.01852836, -0.03180068, -0.07411851, ..., -0.00197019,\n",
       "          0.01199768,  0.01130261],\n",
       "        [-0.01416574,  0.04564881,  0.05952034, ..., -0.03004984,\n",
       "         -0.02417881, -0.04183076]], dtype=float32),\n",
       " array([[ 0.06701853, -0.04063964, -0.06178867, ...,  0.01089184,\n",
       "         -0.01366578, -0.02568761],\n",
       "        [ 0.0846475 ,  0.00272666, -0.06455816, ...,  0.04696646,\n",
       "         -0.06039636, -0.00335864],\n",
       "        [ 0.03886199, -0.02831236, -0.02234175, ...,  0.00904637,\n",
       "         -0.02847461, -0.0095209 ]], dtype=float32))"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Student task: Write a semantically different prediction sentence and compute embeddings\n",
    "# Complete the sections with **********\n",
    "\n",
    "labels = [\"Cusco is in Peru\", \"Ayacucho is a region\", \"Trujillo beaches are marvelous\"]\n",
    "preds = [\n",
    "    \"Peru includes Cusco\",\n",
    "    \"Ayacucho is a department\",\n",
    "    # Write a sentence that is very semantically different from the prediction\n",
    "    # \"***********\"\n",
    "    # <<< START SOLUTION SECTION\n",
    "    \"Turquoise is a marvelous color\",\n",
    "    # >>> END SOLUTION SECTION\n",
    "]\n",
    "\n",
    "\n",
    "# Get the embeddings for each sentence\n",
    "pred_embeddings = \"**********\"\n",
    "label_embeddings = \"**********\"\n",
    "\n",
    "# <<< START SOLUTION SECTION\n",
    "pred_embeddings = model.encode(preds)\n",
    "label_embeddings = model.encode(labels)\n",
    "# >>> END SOLUTION SECTION\n",
    "\n",
    "assert pred_embeddings.shape == (3, 384), (\n",
    "    f\"Expected shape (3, 384), got {pred_embeddings.shape}\"\n",
    ")\n",
    "assert label_embeddings.shape == (3, 384), (\n",
    "    f\"Expected shape (3, 384), got {label_embeddings.shape}\"\n",
    ")\n",
    "\n",
    "pred_embeddings, label_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "6e3b346c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pair 1:\n",
      "  Pred: Peru includes Cusco\n",
      "  Label: Cusco is in Peru\n",
      "  Cosine Similarity: 0.9358\n",
      "\n",
      "Pair 2:\n",
      "  Pred: Ayacucho is a department\n",
      "  Label: Ayacucho is a region\n",
      "  Cosine Similarity: 0.7663\n",
      "\n",
      "Pair 3:\n",
      "  Pred: Turquoise is a marvelous color\n",
      "  Label: Trujillo beaches are marvelous\n",
      "  Cosine Similarity: 0.2680\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate the cosine similarity for each pair of embeddings\n",
    "# No changes needed in this cell, but if it fails, check the above cell\n",
    "\n",
    "cosine_similarity = [\n",
    "    # Cosine similarity for two vectors a and b is defined as:\n",
    "    # cos_sim(a, b) = (a . b) / (||a|| * ||b||)\n",
    "    # where (a . b) is the dot product of a and b,\n",
    "    # and ||a|| and ||b|| are the magnitudes (norms) of vectors a and b respectively.\n",
    "    float(\n",
    "        np.dot(pred_embeddings[i], label_embeddings[i])\n",
    "        / np.linalg.norm(pred_embeddings[i])\n",
    "        / np.linalg.norm(label_embeddings[i])\n",
    "    )\n",
    "    for i in range(len(preds))\n",
    "]\n",
    "\n",
    "# Compute cosine similarity between the two embeddings\n",
    "for i, (p, l, cos_sim) in enumerate(zip(preds, labels, cosine_similarity)):\n",
    "    print(f\"Pair {i + 1}:\")\n",
    "    print(f\"  Pred: {p}\")\n",
    "    print(f\"  Label: {l}\")\n",
    "    print(f\"  Cosine Similarity: {cos_sim:.4f}\\n\")\n",
    "\n",
    "# Check that the last pair has the lowest similarity\n",
    "assert cosine_similarity[-1] < cosine_similarity[0], (\n",
    "    \"The last pair should have the lowest cosine similarity. Please check your prediction sentence.\"\n",
    ")\n",
    "assert cosine_similarity[-1] < cosine_similarity[1], (\n",
    "    \"The last pair should have the lowest cosine similarity. Please check your prediction sentence.\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bbaf675",
   "metadata": {},
   "source": [
    "## Functional Correctness\n",
    "Let's evaluate code-generation by running a tiny function against unit tests (execution accuracy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "6ecc358d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportion of tests passed: 0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "# Student task: Complete the evaluation of the sort_and_normalize function\n",
    "# Complete the sections with **********\n",
    "\n",
    "\n",
    "def sort_and_normalize(s: str) -> str:\n",
    "    \"\"\"Sort the words in the string\"\"\"\n",
    "\n",
    "    # Our toy function will fail on this edge case\n",
    "    if \"armadillo\" in s:\n",
    "        s = s.replace(\"armadillo\", \"kitty\")\n",
    "\n",
    "    return \" \".join(sorted(s.split()))\n",
    "\n",
    "\n",
    "preds = [\n",
    "    \"the capybara is the largest rodent\",\n",
    "    \"an armadillo has a hard shell\",\n",
    "    \"elephants are the largest land animals\",\n",
    "]\n",
    "labels = [\n",
    "    \"capybara is largest rodent the the\",\n",
    "    \"a an armadillo hard has shell\",\n",
    "    \"animals are elephants land largest the\",\n",
    "]\n",
    "\n",
    "# Write tests to check if sort_and_normalize works correctly\n",
    "results = [\n",
    "    # \"**********\"\n",
    "    # <<< START SOLUTION SECTION\n",
    "    sort_and_normalize(p) == l\n",
    "    # >>> END SOLUTION SECTION\n",
    "    for p, l in zip(preds, labels)\n",
    "]\n",
    "\n",
    "print(\"Proportion of tests passed:\", sum(results) / len(results))\n",
    "\n",
    "assert sum(results) == 2, (\n",
    "    f\"2 tests should pass, but got {sum(results)}. Please check how your are evaluating the results.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5b5ab6",
   "metadata": {},
   "source": [
    "## Pass@k\n",
    "\n",
    "Let's simulate multiple samples for a single task and compute pass@k (1 if any sample equals the gold)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "e5459f06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pass@4 = 1\n"
     ]
    }
   ],
   "source": [
    "# Student task: Implement pass_at_k\n",
    "# Complete the sections with **********\n",
    "\n",
    "label = \"Lima\"\n",
    "samples = [\"Lima\", \"Arequipa\", \"Cusco\", \"Lima\"]\n",
    "\n",
    "\n",
    "# Implement pass_at_k with signature (samples: List[str], label: str) -> int\n",
    "# **********\n",
    "\n",
    "\n",
    "# <<< START SOLUTION SECTION\n",
    "from typing import List\n",
    "def pass_at_k(samples: List[str], label: str) -> int:\n",
    "    return int(any(s == label for s in samples))\n",
    "\n",
    "\n",
    "# >>> END SOLUTION SECTION\n",
    "\n",
    "print(\"pass@4 =\", pass_at_k(samples, label))\n",
    "\n",
    "assert pass_at_k(samples, label) == 1, (\n",
    "    f\"pass@4 should be 1, but got {pass_at_k(samples, label)}. Please check your pass_at_k function.\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b336e1",
   "metadata": {},
   "source": [
    "## LLM as a Judge\n",
    "\n",
    "Let's create a function that calls an LLM to compare predicted values and reference values (if applicable) and return a score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "97697d6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM response: <reasoning>Manila is the capital city of the Philippines, which exactly matches the label value. Therefore, the prediction is correct.</reasoning>\n",
      "<score>1.0</score>\n",
      "LLM response: <reasoning> The label is Philippines (a country). The prediction Cebu is a city in the Philippines but not the capital (Manila). Therefore it matches the country but not the capital, yielding a score of 0.5. </reasoning>\n",
      "<score>0.5</score>\n",
      "LLM response: <reasoning>Tokyo is not the capital of the Philippines (the capital is Manila). It is not in the same country as the label, so the prediction is incorrect.</reasoning>\n",
      "<score>0.0</score>\n"
     ]
    }
   ],
   "source": [
    "# Student task: Complete the LLM-as-a-judge function\n",
    "# Complete the sections with **********\n",
    "\n",
    "\n",
    "def llm_as_judge(pred: str, rubric: str, label: str | None = None) -> float:\n",
    "    \"\"\"Use an LLM to judge the quality of a prediction against a rubric and optional label.\"\"\"\n",
    "    from litellm import completion\n",
    "\n",
    "    # Write a system prompt that instructs the LLM to use the rubric to score the prediction\n",
    "    # The response should be formatted as:\n",
    "    # <reasoning>...</reasoning>\n",
    "    # <score>FLOAT_ANSWER</score>\n",
    "    # where FLOAT_ANSWER is a float between 0 and 1.\n",
    "    # We will extract FLOAT_ANSWER from the response later\n",
    "\n",
    "    SYSTEM_PROMPT = \"**********\"\n",
    "\n",
    "    # <<< START SOLUTION SECTION\n",
    "    SYSTEM_PROMPT = f\"\"\"You are an expert evaluator. Use the following rubric to score the prediction.\n",
    "    Format your response as:\n",
    "    <reasoning>...</reasoning>\n",
    "    <score>FLOAT_ANSWER</score>\n",
    "\n",
    "    where FLOAT_ANSWER is a float between 0 and 1.\n",
    "\n",
    "    RUBRIC:\n",
    "    {rubric}\n",
    "    \"\"\"\n",
    "    # >>> END SOLUTION SECTION\n",
    "\n",
    "    # Create a user prompt with the prediction and, optionally, the label\n",
    "    # **********\n",
    "\n",
    "    # <<< START SOLUTION SECTION\n",
    "    USER_PROMPT = f\"Prediction: {pred}\\n\"\n",
    "    if label is not None:\n",
    "        USER_PROMPT += f\"Label: {label}\\n\"\n",
    "    # >>> END SOLUTION SECTION\n",
    "\n",
    "    # Call the LLM using litellm with the system and user prompts (use the model gpt-5-nano)\n",
    "    # See: https://github.com/BerriAI/litellm\n",
    "\n",
    "    # response = completion(**********)\n",
    "\n",
    "    # <<< START SOLUTION SECTION\n",
    "    response = completion(\n",
    "        model=\"gpt-5-nano\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": USER_PROMPT},\n",
    "        ],\n",
    "    )\n",
    "    # >>> END SOLUTION SECTION\n",
    "\n",
    "    text_response = response[\"choices\"][0][\"message\"][\"content\"]\n",
    "    print(\"LLM response:\", text_response)\n",
    "\n",
    "    # Extract FLOAT_ANSWER from the response\n",
    "\n",
    "    # float_answer = **********\n",
    "\n",
    "    # <<< START SOLUTION SECTION\n",
    "    float_answer = float(\n",
    "        text_response.split(\"<score>\")[-1].split(\"</score>\")[0].strip()\n",
    "    )\n",
    "    # >>> END SOLUTION SECTION\n",
    "\n",
    "    return float_answer\n",
    "\n",
    "\n",
    "# Write a rubric for evaluating if the prediction is the capital of the label country\n",
    "# 1.0 if correct, 0.5 if a city in the same country, 0.0 otherwise\n",
    "\n",
    "# **********\n",
    "\n",
    "# <<< START SOLUTION SECTION\n",
    "RUBRIC = \"\"\"\n",
    "* Return 1.0 if the prediction is the capital of the label value,\n",
    "* Otherwise, 0.5 if the prediction is a city in the same country as the label value,\n",
    "* Otherwise, return 0.0\n",
    "\"\"\"\n",
    "# >>> END SOLUTION SECTION\n",
    "\n",
    "assert (\n",
    "    llm_as_judge(\n",
    "        pred=\"Manila\",\n",
    "        label=\"Philippines\",\n",
    "        rubric=RUBRIC,\n",
    "    )\n",
    "    == 1.0\n",
    "), \"Manila is the capital of the Philippines\"\n",
    "\n",
    "assert (\n",
    "    llm_as_judge(\n",
    "        pred=\"Cebu\",\n",
    "        label=\"Philippines\",\n",
    "        rubric=RUBRIC,\n",
    "    )\n",
    "    == 0.5\n",
    "), \"Cebu is a city in the Philippines, but not the capital\"\n",
    "\n",
    "assert (\n",
    "    llm_as_judge(\n",
    "        pred=\"Tokyo\",\n",
    "        label=\"Philippines\",\n",
    "        rubric=RUBRIC,\n",
    "    )\n",
    "    == 0.0\n",
    "), \"Tokyo is not in the Philippines\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa792f32",
   "metadata": {},
   "source": [
    "Congrats! You have completed the evaluation exercise. Proper evaluation is the bedrock for building reliable AI systems. Great job! üëèüëèüëè"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4089a86",
   "metadata": {},
   "source": [
    "<br /><br /><br /><br /><br /><br /><br /><br /><br />"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
