# Implementing Evaluations for Generative AI Models - Exercises

This directory contains hands-on exercises for building comprehensive evaluation frameworks for generative AI systems. The exercises focus on implementing multiple evaluation metrics and assessment methodologies.

## Exercise: Gen AI Evaluation Medley

### Overview
In this exercise, you will build a comprehensive evaluation suite that tests generative AI models across multiple dimensions including fluency, relevance, safety, and factual accuracy. You'll implement both automated metrics and human evaluation methodologies.

## Folder Structure

```
exercises
    |_ starter
    |   |_ gen-ai-evaluation-medley-starter.ipynb
    |   |_ README.md
    |_ solution
    |   |_ gen-ai-evaluation-medley-solution.ipynb
    |   |_ demo.ipynb
    |   |_ README.md
    |_ README.md
```

### Exercise Structure
- **Starter Materials** (`starter/`): Contains evaluation framework starter code and sample datasets
- **Solution Materials** (`solution/`): Complete evaluation pipeline with multiple metric implementations