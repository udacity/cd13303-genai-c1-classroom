# Generating Text Using LLMs

This module explores the fundamentals of text generation using Large Language Models (LLMs). You will learn how language models work, understand tokenization processes, and implement various text generation strategies including sampling techniques and prompt engineering.

## Folder Structure

```
> tree
.
|-- README.md
`-- exercises
    |-- README.md
    |-- solution
    |   `-- README.md
    `-- starter
        `-- README.md

4 directories, 4 files
```

## Directory Map
- `exercises/README.md` – overview of text generation concepts and exercise structure
- `exercises/starter/` – text generation implementation starter code and examples
- `exercises/solution/` – complete text generation system with multiple sampling strategies

## Additional Resources
- [The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/) - Visual guide to transformer architecture
- [The Curious Case of Neural Text Degeneration](https://arxiv.org/abs/1904.09751) - Research on sampling strategies
- [Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165) - GPT-3 paper on in-context learning

