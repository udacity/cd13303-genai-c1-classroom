{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo: Generating Text One Token at a Time\n",
    "\n",
    "This notebook provides a brief demonstration of how a large language model (LLM) generates text. We will walk through the process of loading a model, tokenizing text, and generating new text step-by-step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1. Load a Tokenizer and a Model\n",
    "\n",
    "First, we need to load a pre-trained model and its corresponding tokenizer from the Hugging Face `transformers` library. The **tokenizer** converts text into a sequence of numbers (tokens) that the model can process, and the **model** will perform the text generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# For this demo, we'll use 'distilgpt2', a smaller and faster version of GPT-2\n",
    "model_name = \"distilgpt2\"\n",
    "\n",
    "# Load the tokenizer and model associated with our chosen model name\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2. Examine the Tokenization\n",
    "\n",
    "Let's see how the tokenizer converts a simple sentence into a list of token IDs. This process is called **tokenization**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt text: Machine learning is a field of\n",
      "Token IDs: tensor([[37573,  4673,   318,   257,  2214,   286]])\n"
     ]
    }
   ],
   "source": [
    "# Define a starting phrase, also known as a prompt\n",
    "prompt_text = \"Machine learning is a field of\"\n",
    "\n",
    "# Use the tokenizer to convert the text prompt into input tensors for the model\n",
    "inputs = tokenizer(prompt_text, return_tensors=\"pt\")\n",
    "\n",
    "# The 'input_ids' are the numerical representations of our text\n",
    "print(\"Prompt text:\", prompt_text)\n",
    "print(\"Token IDs:\", inputs[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To better understand the tokenization, we can decode each ID back into its corresponding text. Notice that some tokens are whole words, while others are parts of words or punctuation. This is called **subword tokenization**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ID     Token\n",
      "37573   Machine\n",
      " 4673  learning\n",
      "  318        is\n",
      "  257         a\n",
      " 2214     field\n",
      "  286        of\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Get the list of token IDs from our inputs\n",
    "token_ids = inputs[\"input_ids\"][0].tolist()\n",
    "\n",
    "# Decode each token ID back to its string representation\n",
    "tokens = [tokenizer.decode(token_id) for token_id in token_ids]\n",
    "\n",
    "# Display the IDs and their corresponding tokens in a table for clarity\n",
    "token_df = pd.DataFrame({\"ID\": token_ids, \"Token\": tokens})\n",
    "\n",
    "print(token_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3. Generate the Next Token\n",
    "\n",
    "Now, we'll feed our tokenized prompt to the model and ask it to predict the single most likely next token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most likely next token ID is: 2267\n",
      "This token is: ' research'\n"
     ]
    }
   ],
   "source": [
    "# We use torch.no_grad() to disable gradient calculations, as we are not training the model\n",
    "with torch.no_grad():\n",
    "    # Get the model's raw output, called 'logits'\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "    # We only care about the logits for the very last token in our input sequence\n",
    "    next_token_logits = outputs.logits[:, -1, :]\n",
    "\n",
    "    # Convert logits into probabilities using the softmax function\n",
    "    probabilities = torch.nn.functional.softmax(next_token_logits, dim=-1)\n",
    "\n",
    "    # Find the token ID with the highest probability\n",
    "    most_likely_next_token_id = torch.argmax(probabilities).item()\n",
    "\n",
    "print(f\"The most likely next token ID is: {most_likely_next_token_id}\")\n",
    "print(f\"This token is: '{tokenizer.decode(most_likely_next_token_id)}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By repeatedly predicting the next token and appending it to our input, we can generate a longer sequence of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 5 tokens one at a time:\n",
      "Machine learning is a field of research that has been growing"
     ]
    }
   ],
   "source": [
    "# Let's generate a few more tokens by repeating the process in a loop\n",
    "generated_ids = inputs[\"input_ids\"]\n",
    "\n",
    "print(\"Generating 5 tokens one at a time:\")\n",
    "print(tokenizer.decode(generated_ids[0]), end=\"\")\n",
    "\n",
    "# This loop generates one token at a time\n",
    "for _ in range(5):\n",
    "    with torch.no_grad():\n",
    "        outputs = model(generated_ids)\n",
    "        next_token_logits = outputs.logits[:, -1, :]\n",
    "        next_token_id = torch.argmax(next_token_logits, dim=-1).unsqueeze(-1)\n",
    "\n",
    "    # Append the newly predicted token ID to our sequence\n",
    "    generated_ids = torch.cat([generated_ids, next_token_id], dim=-1)\n",
    "\n",
    "    # Print the newly generated token\n",
    "    print(tokenizer.decode(next_token_id[0]), end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4. Use the `generate` Method\n",
    "\n",
    "Generating tokens one-by-one manually is great for understanding the process, but it's inefficient. The `transformers` library provides a convenient `.generate()` method that handles this entire process for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Text Generated with model.generate() ---\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Machine learning is a field of research that has been growing in the past few years.\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "# We start with the same tokenized prompt\n",
    "inputs = tokenizer(prompt_text, return_tensors=\"pt\")\n",
    "\n",
    "# Use the .generate() method to create a sequence of a desired length\n",
    "output_ids = model.generate(\n",
    "    **inputs, max_length=50, pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "# Decode the entire sequence of token IDs into a single string\n",
    "generated_text = tokenizer.decode(output_ids[0])\n",
    "\n",
    "print(\"--- Text Generated with model.generate() ---\")\n",
    "display(Markdown(generated_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This demo shows the core logic of how a language model generates text. Now you're ready to try it yourself!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br /><br /><br /><br /><br /><br /><br /><br /><br />"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
